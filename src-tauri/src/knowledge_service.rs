use crate::database::DatabaseManager;
use crate::vector_service::VectorService;
use crate::types::*;
use anyhow::{Result, anyhow};
use tracing::warn;
use std::sync::Arc;
use chrono::Utc;
use sqlx::Row;
use unicode_segmentation::UnicodeSegmentation;

// æ–‡æ¡£å¤„ç†å™¨
pub struct DocumentProcessor {
    db: Arc<DatabaseManager>,
    vector_service: Arc<VectorService>,
}

impl DocumentProcessor {
    pub fn new(db: Arc<DatabaseManager>, vector_service: Arc<VectorService>) -> Self {
        Self { db, vector_service }
    }

    // å¤„ç†æ–‡æ¡£ï¼ˆå¸¦APIå¯†é’¥ï¼‰
    pub async fn process_document_with_api_key(&self, request: DocumentProcessRequest, api_key: &str) -> Result<DocumentProcessResponse> {
        // åŸºæœ¬å¤„ç†å’Œä¹‹å‰ä¸€æ ·ï¼Œä½†åµŒå…¥ç”Ÿæˆæ—¶ä½¿ç”¨APIå¯†é’¥
        let start_time = std::time::Instant::now();

        // è·å–é›†åˆé…ç½®
        let collection = self.get_collection(&request.collection_id).await?;

        // è·å–ç³»ç»Ÿé…ç½®
        let config = self.get_system_config().await?;

        // éªŒè¯æ–‡æ¡£å¤§å°
        if let Some(size) = request.file_size {
            if size > 10485760 as i64 {
                return Err(anyhow!("Document size exceeds maximum limit"));
            }
        }

        // åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰æ–‡æ¡£è®°å½•
        let document = if let Some(existing_id) = &request.document_id {
            // ä½¿ç”¨ç°æœ‰æ–‡æ¡£
            println!("ğŸ“ ä½¿ç”¨ç°æœ‰æ–‡æ¡£å¤„ç†: {}", existing_id);
            self.db.get_document_by_id(existing_id).await
                .map_err(|e| anyhow!("è·å–ç°æœ‰æ–‡æ¡£å¤±è´¥: {}", e))?
        } else {
            // åˆ›å»ºæ–°æ–‡æ¡£è®°å½•
            println!("ğŸ“ åˆ›å»ºæ–°æ–‡æ¡£è®°å½•: {}", request.title);
            let new_doc = KnowledgeDocument::new(
                request.collection_id.clone(),
                request.title.clone(),
                request.content.clone(),
                request.file_name.clone(),
                request.file_size,
                request.mime_type.clone(),
            );

            // æ‰¹é‡æ’å…¥æ•°æ® - åˆ›å»ºæ–‡æ¡£
            self.db.create_document(&new_doc).await?;
            new_doc
        };

        // æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ chunksï¼Œé¿å…é‡å¤åˆ›å»º
        let existing_chunks = self.db.get_chunks_by_document_id(&document.id).await
            .map_err(|e| anyhow!("è·å–ç°æœ‰chunkså¤±è´¥: {}", e))?;

        if !existing_chunks.is_empty() {
            println!("ğŸ“„ æ–‡æ¡£ {} å·²æœ‰ {} ä¸ªchunksï¼Œè·³è¿‡é‡å¤åˆ›å»º", document.id, existing_chunks.len());

            // è¿”å›ç°æœ‰chunksçš„ç»Ÿè®¡ä¿¡æ¯
            return Ok(DocumentProcessResponse {
                document_id: document.id,
                chunks_count: existing_chunks.len(),
                vectors_count: existing_chunks.len(), // å‡è®¾æ¯ä¸ªchunkéƒ½æœ‰å¯¹åº”çš„vector
                processing_time_ms: start_time.elapsed().as_millis() as u64,
            });
        }

        // åˆ†å—å¤„ç†ï¼šæŒ‰æ¨¡å‹é‡‡ç”¨æ¨è chunk å‚æ•°ï¼ˆè¯·æ±‚æœªæ˜¾å¼æä¾›æ—¶ï¼‰
        let model_id = collection.embedding_model.to_lowercase();
        let mut chunk_size = request.chunk_size.unwrap_or(config.chunk_size);
        let mut chunk_overlap = request.chunk_overlap.unwrap_or(config.chunk_overlap);
        if request.chunk_size.is_none() {
            chunk_size = if model_id.contains("bge-m3") {
                900 // å»ºè®® 800-1024ï¼Œå–ä¸­ä½åä¸Š
            } else if model_id.contains("bge-large-zh") {
                480 // å®‰å…¨ä¸Šé™ï¼Œé¿å…è¶…è¿‡512 tokens
            } else if model_id.contains("bge-large-en") {
                900 // å»ºè®® 800-1024
            } else { chunk_size };
        }
        if request.chunk_overlap.is_none() {
            chunk_overlap = if model_id.contains("bge-m3") {
                120 // å»ºè®® 100-150
            } else if model_id.contains("bge-large-zh") {
                80 // å»ºè®® 50-100
            } else if model_id.contains("bge-large-en") {
                100 // å»ºè®® 80-120
            } else { chunk_overlap };
        }
        println!("ğŸ§© [åˆ†å—å‚æ•°] æ¨¡å‹: {}, chunk_size: {}, overlap: {}", collection.embedding_model, chunk_size, chunk_overlap);

        let mut chunks = self.chunk_document(&request.content, chunk_size, chunk_overlap).await?;

        // éªŒè¯åˆ†å—æ•°é‡ - æé«˜åˆ°5000ä¸ªå—ï¼Œä½†ç»™å‡ºè­¦å‘Š
        if chunks.len() > 5000 {
            return Err(anyhow!("Document chunk count exceeds maximum limit (5000)"));
        } else if chunks.len() > 1000 {
            println!("âš ï¸  æ–‡æ¡£åˆ†å—æ•°é‡è¾ƒå¤§: {} ä¸ªå—ï¼Œå»ºè®®ä¼˜åŒ–åˆ†å—å‚æ•°", chunks.len());
        }

        // ç”ŸæˆåµŒå…¥å‘é‡ - ä½¿ç”¨APIå¯†é’¥è°ƒç”¨å®é™…æœåŠ¡
        // å®‰å…¨æˆªæ–­ï¼šé¿å…å•æ¡æ–‡æœ¬è¶…å‡ºæ¨¡å‹ token é™åˆ¶å¯¼è‡´ 413
        // ä»¥å­—ç¬¦è¿‘ä¼¼ token é™åˆ¶ï¼šCJK 1å­—ç¬¦â‰ˆ1tokenï¼Œå…¶ä»– 4å­—ç¬¦â‰ˆ1tokenã€‚ç›®æ ‡â‰¤512 tokens
        let safe_texts: Vec<String> = chunks.iter().map(|c| {
            let s = c.chunk_text.as_str();
            let is_cjk = s.chars().any(|ch| (ch >= '\u{4E00}' && ch <= '\u{9FFF}') || (ch >= '\u{3400}' && ch <= '\u{4DBF}'));
            let max_chars = if is_cjk { 512 } else { 2048 };
            let count = s.chars().count();
            if count > max_chars { s.chars().take(max_chars).collect::<String>() } else { s.to_string() }
        }).collect();

        let embeddings = self.generate_embeddings_with_api_key(
            &safe_texts,
            &collection.embedding_model,
            api_key
        ).await?;

        // æ›´æ–°æ‰€æœ‰å—çš„document_idä¸ºå®é™…ID
        for chunk in &mut chunks {
            chunk.document_id = document.id.clone();
        }

        let chunk_ids = self.db.create_chunks(&chunks).await?;

        // åˆ›å»ºå‘é‡è®°å½•
        let vector_embeddings: Vec<VectorEmbedding> = chunk_ids.iter().zip(embeddings.iter())
            .map(|(chunk_id, embedding)| VectorEmbedding::new(
                *chunk_id,
                collection.id.clone(),
                embedding.clone(),
            ))
            .collect();

        self.db.insert_vectors(&vector_embeddings).await?;

        let processing_time = start_time.elapsed();

        Ok(DocumentProcessResponse {
            document_id: document.id,
            chunks_count: chunks.len(),
            vectors_count: vector_embeddings.len(),
            processing_time_ms: processing_time.as_millis() as u64,
        })
    }

    // ç”Ÿæˆå¸¦APIå¯†é’¥çš„åµŒå…¥å‘é‡
    pub async fn generate_embeddings_with_api_key(&self, texts: &[String], model_id: &str, api_key: &str) -> Result<Vec<Vec<f32>>> {
        let model = self.vector_service.get_embedding_model(model_id).await?;

        // è°ƒç”¨å®é™…çš„åµŒå…¥æœåŠ¡ï¼Œä½¿ç”¨æä¾›çš„APIå¯†é’¥
        let embeddings = self.vector_service.generate_embeddings_with_api_key_batch(texts, &model, api_key).await?;

        Ok(embeddings)
    }

    // ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆå¸¦APIå¯†é’¥ï¼‰
    pub async fn generate_embedding_with_api_key(&self, text: &str, model_id: &str, api_key: &str) -> Result<Vec<f32>> {
        let embeddings = self.generate_embeddings_with_api_key(&[text.to_string()], model_id, api_key).await?;
        Ok(embeddings.into_iter().next().unwrap_or_default())
    }

    // å¤„ç†æ–‡æ¡£
    pub async fn process_document(&self, request: DocumentProcessRequest) -> Result<DocumentProcessResponse> {
        let start_time = std::time::Instant::now();

        // è·å–é›†åˆé…ç½®
        let collection = self.get_collection(&request.collection_id).await?;

        // è·å–ç³»ç»Ÿé…ç½®
        let config = self.get_system_config().await?;

        // éªŒè¯æ–‡æ¡£å¤§å°
        if let Some(size) = request.file_size {
            if size > 10485760 as i64 {
                return Err(anyhow!("Document size exceeds maximum limit"));
            }
        }

        // åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰æ–‡æ¡£è®°å½•
        let document = if let Some(existing_id) = &request.document_id {
            // ä½¿ç”¨ç°æœ‰æ–‡æ¡£
            println!("ğŸ“ ä½¿ç”¨ç°æœ‰æ–‡æ¡£å¤„ç†: {}", existing_id);
            self.db.get_document_by_id(existing_id).await
                .map_err(|e| anyhow!("è·å–ç°æœ‰æ–‡æ¡£å¤±è´¥: {}", e))?
        } else {
            // åˆ›å»ºæ–°æ–‡æ¡£è®°å½•
            println!("ğŸ“ åˆ›å»ºæ–°æ–‡æ¡£è®°å½•: {}", request.title);
            let new_doc = KnowledgeDocument::new(
                request.collection_id.clone(),
                request.title.clone(),
                request.content.clone(),
                request.file_name.clone(),
                request.file_size,
                request.mime_type.clone(),
            );

            // æ‰¹é‡æ’å…¥æ•°æ® - åˆ›å»ºæ–‡æ¡£
            self.db.create_document(&new_doc).await?;
            new_doc
        };

        // åˆ†å—å¤„ç†ï¼šæŒ‰æ¨¡å‹é‡‡ç”¨æ¨è chunk å‚æ•°ï¼ˆè¯·æ±‚æœªæ˜¾å¼æä¾›æ—¶ï¼‰
        let model_id = collection.embedding_model.to_lowercase();
        let mut chunk_size = request.chunk_size.unwrap_or(config.chunk_size);
        let mut chunk_overlap = request.chunk_overlap.unwrap_or(config.chunk_overlap);
        if request.chunk_size.is_none() {
            chunk_size = if model_id.contains("bge-m3") {
                900
            } else if model_id.contains("bge-large-zh") {
                640
            } else if model_id.contains("bge-large-en") {
                900
            } else { chunk_size };
        }
        if request.chunk_overlap.is_none() {
            chunk_overlap = if model_id.contains("bge-m3") {
                120
            } else if model_id.contains("bge-large-zh") {
                80
            } else if model_id.contains("bge-large-en") {
                100
            } else { chunk_overlap };
        }
        println!("ğŸ§© [åˆ†å—å‚æ•°] æ¨¡å‹: {}, chunk_size: {}, overlap: {}", collection.embedding_model, chunk_size, chunk_overlap);

        let mut chunks = self.chunk_document(&request.content, chunk_size, chunk_overlap).await?;

        // éªŒè¯åˆ†å—æ•°é‡ - æé«˜åˆ°5000ä¸ªå—ï¼Œä½†ç»™å‡ºè­¦å‘Š
        if chunks.len() > 5000 {
            return Err(anyhow!("Document chunk count exceeds maximum limit (5000)"));
        } else if chunks.len() > 1000 {
            println!("âš ï¸  æ–‡æ¡£åˆ†å—æ•°é‡è¾ƒå¤§: {} ä¸ªå—ï¼Œå»ºè®®ä¼˜åŒ–åˆ†å—å‚æ•°", chunks.len());
        }

        // ç”ŸæˆåµŒå…¥å‘é‡
        let safe_texts: Vec<String> = chunks.iter().map(|c| {
            let s = c.chunk_text.as_str();
            let is_cjk = s.chars().any(|ch| (ch >= '\u{4E00}' && ch <= '\u{9FFF}') || (ch >= '\u{3400}' && ch <= '\u{4DBF}'));
            let max_chars = if is_cjk { 512 } else { 2048 };
            let count = s.chars().count();
            if count > max_chars { s.chars().take(max_chars).collect::<String>() } else { s.to_string() }
        }).collect();

        let embeddings = self.vector_service.generate_embeddings_batch(
            &safe_texts,
            &collection.embedding_model,
        ).await?;

        // æ›´æ–°æ‰€æœ‰å—çš„document_idä¸ºå®é™…ID
        for chunk in &mut chunks {
            chunk.document_id = document.id.clone();
        }

        let chunk_ids = self.db.create_chunks(&chunks).await?;

        // åˆ›å»ºå‘é‡è®°å½•
        let vector_embeddings: Vec<VectorEmbedding> = chunk_ids.iter().zip(embeddings.iter())
            .map(|(chunk_id, embedding)| VectorEmbedding::new(
                *chunk_id,
                collection.id.clone(),
                embedding.clone(),
            ))
            .collect();

        self.db.insert_vectors(&vector_embeddings).await?;

        let processing_time = start_time.elapsed();

        Ok(DocumentProcessResponse {
            document_id: document.id,
            chunks_count: chunks.len(),
            vectors_count: vector_embeddings.len(),
            processing_time_ms: processing_time.as_millis() as u64,
        })
    }

    // æ–‡æ¡£åˆ†å—
    async fn chunk_document(&self, content: &str, chunk_size: usize, chunk_overlap: usize) -> Result<Vec<KnowledgeChunk>> {
        let mut chunks = Vec::new();
        let graphemes = content.graphemes(true).collect::<Vec<_>>();
        let total_chars = graphemes.len();

        if total_chars == 0 {
            return Ok(chunks);
        }

        let mut start = 0;
        let mut chunk_index = 0;

        while start < total_chars {
            let end = (start + chunk_size).min(total_chars);
            let chunk_text = graphemes[start..end].concat();

            // è®¡ç®—tokenæ•°é‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            let token_count = (chunk_text.len() + 3) / 4; // ç²—ç•¥ä¼°ç®—

            chunks.push(KnowledgeChunk::new(
                "temp_doc_id".to_string(), // å°†åœ¨æ’å…¥æ—¶æ›¿æ¢ä¸ºå®é™…ID
                chunk_index,
                chunk_text,
                token_count as i32,
            ));

            start = if end >= total_chars {
                total_chars
            } else {
                end.saturating_sub(chunk_overlap)
            };

            chunk_index += 1;
        }

        Ok(chunks)
    }

    // è·å–é›†åˆ
    async fn get_collection(&self, collection_id: &str) -> Result<KnowledgeCollection> {
        let row = sqlx::query(
            "SELECT id, name, description, embedding_model, vector_dimensions, created_at, updated_at
             FROM knowledge_collections WHERE id = ?"
        )
        .bind(collection_id)
        .fetch_one(self.db.knowledge_pool())
        .await?;

        Ok(KnowledgeCollection {
            id: row.get(0),
            name: row.get(1),
            description: row.get(2),
            embedding_model: row.get(3),
            vector_dimensions: row.get(4),
            created_at: chrono::DateTime::from_timestamp(row.get(5), 0).unwrap_or_default(),
            updated_at: chrono::DateTime::from_timestamp(row.get(6), 0).unwrap_or_default(),
        })
    }

    // è·å–ç³»ç»Ÿé…ç½®
    async fn get_system_config(&self) -> Result<SystemConfig> {
        let config_items = vec![
            "chunk_size", "chunk_overlap", "search_limit", "similarity_threshold",
            "cache_ttl",
        ];

        let mut config = SystemConfig::default();

        for key in config_items {
            if let Ok(Some(value)) = self.db.get_config(key).await {
                match key {
                    "chunk_size" => config.chunk_size = value.parse().unwrap_or(500),
                    "chunk_overlap" => config.chunk_overlap = value.parse().unwrap_or(50),
                    "search_limit" => config.search_limit = value.parse().unwrap_or(10),
                    "similarity_threshold" => config.similarity_threshold = value.parse().unwrap_or(0.7),
                    "cache_ttl" => config.cache_ttl = value.parse().unwrap_or(3600),
                    _ => {}
                }
            }
        }

        Ok(config)
    }
}

// çŸ¥è¯†åº“æœç´¢æœåŠ¡
pub struct KnowledgeSearchService {
    db: Arc<DatabaseManager>,
    vector_service: Arc<VectorService>,
}

impl KnowledgeSearchService {
    pub fn new(db: Arc<DatabaseManager>, vector_service: Arc<VectorService>) -> Self {
        Self { db, vector_service }
    }

    // æœç´¢çŸ¥è¯†åº“
    pub async fn search(&self, request: SearchRequest) -> Result<SearchResponse> {
        let start_time = std::time::Instant::now();

        // è°ƒè¯•ï¼šæ‰“å°è¯·æ±‚ä¿¡æ¯
        println!("ğŸ” [è°ƒè¯•] æ”¶åˆ°æœç´¢è¯·æ±‚: query='{}', collection_id={:?}", request.query, request.collection_id);

        // è·å–ç³»ç»Ÿé…ç½®
        let config = self.get_system_config().await?;
        println!("ğŸ” [è°ƒè¯•] ç³»ç»Ÿé…ç½®: default_collection='{}'", config.default_collection);

        // ç¡®å®šæœç´¢é›†åˆ
        let collection_id = request.collection_id.unwrap_or_else(|| {
            println!("ğŸ” [è°ƒè¯•] ä½¿ç”¨é»˜è®¤é›†åˆ: '{}'", config.default_collection);
            config.default_collection.clone()
        });
        println!("ğŸ” [è°ƒè¯•] æœ€ç»ˆä½¿ç”¨é›†åˆID: '{}'", collection_id);

        // è·å–é›†åˆé…ç½®
        let collection = match self.get_collection(&collection_id).await {
            Ok(collection) => collection,
            Err(e) if e.to_string().contains("no rows returned") => {
                return Err(anyhow::anyhow!("é›†åˆ '{}' ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥é›†åˆIDæ˜¯å¦æ­£ç¡®", collection_id));
            }
            Err(e) => return Err(e),
        };

        // è®¾ç½®æœç´¢å‚æ•°
        let limit = request.limit.unwrap_or(config.search_limit);

        // æ¨¡å‹å·®å¼‚åŒ–é»˜è®¤é˜ˆå€¼
        let model_id = collection.embedding_model.to_lowercase();
        let model_default_threshold: f32 = if model_id.contains("bge-m3") {
            0.80
        } else if model_id.contains("bge-large-zh") {
            0.75
        } else if model_id.contains("bge-large-en") {
            0.70
        } else {
            config.similarity_threshold
        };

        // ä½¿ç”¨åˆç†çš„é˜ˆå€¼è®¾ç½®
        let mut threshold = request.threshold.unwrap_or(0.3); // ä½¿ç”¨åˆç†çš„é˜ˆå€¼
        println!("ğŸ”§ [é˜ˆå€¼è°ƒæ•´] ä½¿ç”¨é˜ˆå€¼: {:.3}", threshold);

        // ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆbge-large-zh éœ€è¦åŠ å®˜æ–¹æŸ¥è¯¢æŒ‡ä»¤å‰ç¼€ï¼‰
        let mut query_text = request.query.clone();
        if model_id.contains("bge-large-zh") {
            let prefix = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š";
            query_text = format!("{}{}", prefix, query_text);
            println!("ğŸ§© [æŸ¥è¯¢æŒ‡ä»¤] ä½¿ç”¨ bge-large-zhï¼Œå·²æ·»åŠ æŸ¥è¯¢å‰ç¼€");
        }

        let query_embedding = if !request.api_key.is_empty() {
            println!("ğŸ” ä½¿ç”¨æä¾›çš„APIå¯†é’¥ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå¯†é’¥é•¿åº¦: {}", request.api_key.len());
            
            // =================================================================
            // VVVV å…³é”®çš„è°ƒè¯•æ—¥å¿— VVVV
            // è¯·æŠŠè¿™ä¸€è¡Œæ—¥å¿—åŠ åˆ°ä½ çš„ä»£ç é‡Œ
            println!("[æœ€ç»ˆéªŒè¯] å³å°†è¢«BGE-ZHæ¨¡å‹ç¼–ç çš„å­—ç¬¦ä¸²æ˜¯: '{}'", query_text);
            // ^^^^ å…³é”®çš„è°ƒè¯•æ—¥å¿— ^^^^
            // =================================================================
            
            // ç›´æ¥ä½¿ç”¨vector_serviceçš„æ–¹æ³•
            let model = self.vector_service.get_embedding_model(&collection.embedding_model).await?;
            let embeddings = self.vector_service.generate_embeddings_with_api_key_batch(&[query_text.clone()], &model, &request.api_key).await?;
            embeddings.into_iter().next().unwrap_or_default()
        } else {
            println!("ğŸ” APIå¯†é’¥ä¸ºç©ºï¼Œä½¿ç”¨æ— å¯†é’¥æ–¹å¼ç”ŸæˆæŸ¥è¯¢å‘é‡");
            
            // =================================================================
            // VVVV å…³é”®çš„è°ƒè¯•æ—¥å¿— VVVV
            // è¯·æŠŠè¿™ä¸€è¡Œæ—¥å¿—åŠ åˆ°ä½ çš„ä»£ç é‡Œ
            println!("[æœ€ç»ˆéªŒè¯] å³å°†è¢«BGE-ZHæ¨¡å‹ç¼–ç çš„å­—ç¬¦ä¸²æ˜¯: '{}'", query_text);
            // ^^^^ å…³é”®çš„è°ƒè¯•æ—¥å¿— ^^^^
            // =================================================================
            
            self.vector_service.generate_embedding(&query_text, &collection.embedding_model).await?
        };

        // æ‰§è¡Œå‘é‡æœç´¢
        let mut results = self.db.search_vectors(
            &query_embedding,
            &collection_id,
            limit,
            threshold,
        ).await?;

        // ç©ºç»“æœè‡ªåŠ¨é™é˜ˆå›é€€ï¼šå…ˆ 0.40ï¼Œå† 0.30
        if results.is_empty() {
            let retry_thresholds = [0.40_f32, 0.30_f32];
            for rt in retry_thresholds {
                if threshold > rt { // ä»…å½“å½“å‰é˜ˆå€¼é«˜äºå›é€€é˜ˆå€¼æ—¶æ‰å›é€€
                    println!("ğŸ› ï¸ [å›é€€] åˆæ¬¡æ£€ç´¢æ— ç»“æœï¼Œé™é˜ˆè‡³ {:.2} é‡è¯•", rt);
                    results = self.db.search_vectors(
                        &query_embedding,
                        &collection_id,
                        limit,
                        rt,
                    ).await?;
                    if !results.is_empty() { break; }
                }
            }
        }

        // è®°å½•æœç´¢å†å²
        if self.is_search_history_enabled().await? {
            self.record_search_history(&request.query, &collection_id, results.len(), start_time.elapsed()).await?;
        }

        let query_time = start_time.elapsed();
        let total_count = results.len();

        Ok(SearchResponse {
            results,
            total_count,
            query_time_ms: query_time.as_millis() as u64,
            collection_id,
            embedding_model: collection.embedding_model,
        })
    }

    // å¤šé›†åˆæœç´¢
    pub async fn search_all_collections(&self, request: SearchRequest) -> Result<Vec<SearchResponse>> {
        let collections = self.get_all_collections().await?;
        let mut responses = Vec::new();

        for collection in collections {
            let mut collection_request = request.clone();
            collection_request.collection_id = Some(collection.id.clone());

            match self.search(collection_request).await {
                Ok(response) => {
                    responses.push(response);
                }
                Err(e) => {
                    warn!("Failed to search collection {}: {}", collection.id, e);
                }
            }
        }

        Ok(responses)
    }

    // è·å–æ‰€æœ‰é›†åˆ
    async fn get_all_collections(&self) -> Result<Vec<KnowledgeCollection>> {
        self.db.get_collections().await
    }

    // è·å–é›†åˆ
    async fn get_collection(&self, collection_id: &str) -> Result<KnowledgeCollection> {
        let row = sqlx::query(
            "SELECT id, name, description, embedding_model, vector_dimensions, created_at, updated_at
             FROM knowledge_collections WHERE id = ?"
        )
        .bind(collection_id)
        .fetch_one(self.db.knowledge_pool())
        .await?;

        Ok(KnowledgeCollection {
            id: row.get(0),
            name: row.get(1),
            description: row.get(2),
            embedding_model: row.get(3),
            vector_dimensions: row.get(4),
            created_at: chrono::DateTime::from_timestamp(row.get(5), 0).unwrap_or_default(),
            updated_at: chrono::DateTime::from_timestamp(row.get(6), 0).unwrap_or_default(),
        })
    }

    // è·å–ç³»ç»Ÿé…ç½®
    async fn get_system_config(&self) -> Result<SystemConfig> {
        let config_items = vec![
            "default_collection", "chunk_size", "chunk_overlap", "search_limit",
            "similarity_threshold", "cache_ttl",
        ];

        let mut config = SystemConfig::default();

        for key in config_items {
            if let Ok(Some(value)) = self.db.get_config(key).await {
                match key {
                    "default_collection" => config.default_collection = value,
                    "chunk_size" => config.chunk_size = value.parse().unwrap_or(500),
                    "chunk_overlap" => config.chunk_overlap = value.parse().unwrap_or(50),
                    "search_limit" => config.search_limit = value.parse().unwrap_or(10),
                    "similarity_threshold" => config.similarity_threshold = value.parse().unwrap_or(0.7),
                    "cache_ttl" => config.cache_ttl = value.parse().unwrap_or(3600),
                    _ => {}
                }
            }
        }

        Ok(config)
    }

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨æœç´¢å†å²
    async fn is_search_history_enabled(&self) -> Result<bool> {
        match self.db.get_config("enable_search_history").await? {
            Some(value) => Ok(value.parse().unwrap_or(false)),
            None => Ok(true),
        }
    }

    // è®°å½•æœç´¢å†å²
    async fn record_search_history(&self, query: &str, collection_id: &str, results_count: usize, execution_time: std::time::Duration) -> Result<()> {
        let id = uuid::Uuid::new_v4().to_string();

        sqlx::query(
            "INSERT INTO search_history (id, query_text, collection_id, results_count, execution_time, created_at)
             VALUES (?, ?, ?, ?, ?, ?)"
        )
        .bind(&id)
        .bind(query)
        .bind(collection_id)
        .bind(results_count as i64)
        .bind(execution_time.as_millis() as i64)
        .bind(Utc::now().timestamp())
        .execute(self.db.knowledge_pool())
        .await?;

        Ok(())
    }
}

// çŸ¥è¯†åº“ç®¡ç†æœåŠ¡
pub struct KnowledgeManagementService {
    db: Arc<DatabaseManager>,
    vector_service: Arc<VectorService>,
}

impl KnowledgeManagementService {
    pub fn new(db: Arc<DatabaseManager>, vector_service: Arc<VectorService>) -> Self {
        Self { db, vector_service }
    }

    // è·å–æ‰€æœ‰é›†åˆ
    pub async fn get_collections(&self) -> Result<Vec<KnowledgeCollection>> {
        self.db.get_collections().await
    }

    // åˆ›å»ºé›†åˆ
    pub async fn create_collection(&self, collection: KnowledgeCollection) -> Result<()> {
        self.db.create_collection(&collection).await?;
        Ok(())
    }

    // åˆ é™¤é›†åˆ
    pub async fn delete_collection(&self, collection_id: &str) -> Result<()> {
        let mut tx = self.db.knowledge_pool().begin().await?;

        // åˆ é™¤å‘é‡ï¼ˆé€šè¿‡å…³è”è¡¨åˆ é™¤ï¼‰
        sqlx::query(
            "DELETE FROM knowledge_vectors WHERE rowid IN (
                SELECT kc.id FROM knowledge_chunks kc
                JOIN knowledge_documents kd ON kc.document_id = kd.id
                WHERE kd.collection_id = ?
            )"
        )
        .bind(collection_id)
        .execute(&mut *tx)
        .await?;

        // åˆ é™¤æ–‡æ¡£ï¼ˆçº§è”åˆ é™¤åˆ†å—å’Œå‘é‡ï¼‰
        sqlx::query("DELETE FROM knowledge_documents WHERE collection_id = ?")
            .bind(collection_id)
            .execute(&mut *tx)
            .await?;

        // åˆ é™¤é›†åˆ
        sqlx::query("DELETE FROM knowledge_collections WHERE id = ?")
            .bind(collection_id)
            .execute(&mut *tx)
            .await?;

        tx.commit().await?;

        // æ¸…ç†ç¼“å­˜
        self.db.clear_cache();

        Ok(())
    }

    // è·å–é›†åˆç»Ÿè®¡
    pub async fn get_collection_stats(&self, collection_id: &str) -> Result<CollectionStats> {
        let collection = self.get_collection(collection_id).await?;

        let documents_count = sqlx::query("SELECT COUNT(*) FROM knowledge_documents WHERE collection_id = ?")
            .bind(collection_id)
            .fetch_one(self.db.knowledge_pool())
            .await?
            .get::<i64, _>(0) as usize;

        let chunks_count = sqlx::query(
            "SELECT COUNT(*) FROM knowledge_chunks kc
             JOIN knowledge_documents kd ON kc.document_id = kd.id
             WHERE kd.collection_id = ?"
        )
        .bind(collection_id)
        .fetch_one(self.db.knowledge_pool())
        .await?
        .get::<i64, _>(0) as usize;

        let vectors_count = sqlx::query(
            "SELECT COUNT(*) FROM knowledge_vectors kv
             JOIN knowledge_chunks kc ON kv.rowid = kc.id
             JOIN knowledge_documents kd ON kc.document_id = kd.id
             WHERE kd.collection_id = ?"
        )
        .bind(collection_id)
        .fetch_one(self.db.knowledge_pool())
        .await?
        .get::<i64, _>(0) as usize;

        let total_size_bytes = sqlx::query(
            "SELECT SUM(CAST(LENGTH(kd.content) AS INTEGER)) FROM knowledge_documents kd WHERE kd.collection_id = ?"
        )
        .bind(collection_id)
        .fetch_one(self.db.knowledge_pool())
        .await?
        .get::<Option<i64>, _>(0)
        .unwrap_or(0) as usize;

        Ok(CollectionStats {
            collection_id: collection.id,
            collection_name: collection.name,
            documents_count,
            chunks_count,
            vectors_count,
            total_size_bytes,
            created_at: collection.created_at,
            last_updated: collection.updated_at,
        })
    }

    // è·å–ç³»ç»ŸçŠ¶æ€
    pub async fn get_system_status(&self) -> Result<SystemStatus> {
        let database_health = self.db.health_check().await?;
        let cache_stats = database_health.cache_stats;

        let collections_count = sqlx::query("SELECT COUNT(*) FROM knowledge_collections")
            .fetch_one(self.db.knowledge_pool())
            .await?
            .get::<i64, _>(0) as usize;

        let total_documents = sqlx::query("SELECT COUNT(*) FROM knowledge_documents")
            .fetch_one(self.db.knowledge_pool())
            .await?
            .get::<i64, _>(0) as usize;

        let total_vectors = sqlx::query("SELECT COUNT(*) FROM knowledge_vectors")
            .fetch_one(self.db.knowledge_pool())
            .await?
            .get::<i64, _>(0) as usize;

        let memory_usage = self.get_memory_usage().await?;

        Ok(SystemStatus {
            database_health,
            collections_count,
            total_documents,
            total_vectors,
            uptime_seconds: 0, // éœ€è¦è®°å½•å¯åŠ¨æ—¶é—´
            memory_usage_mb: memory_usage,
            cache_stats,
        })
    }

    // è·å–é›†åˆ
    async fn get_collection(&self, collection_id: &str) -> Result<KnowledgeCollection> {
        let row = sqlx::query(
            "SELECT id, name, description, embedding_model, vector_dimensions, created_at, updated_at
             FROM knowledge_collections WHERE id = ?"
        )
        .bind(collection_id)
        .fetch_one(self.db.knowledge_pool())
        .await?;

        Ok(KnowledgeCollection {
            id: row.get(0),
            name: row.get(1),
            description: row.get(2),
            embedding_model: row.get(3),
            vector_dimensions: row.get(4),
            created_at: chrono::DateTime::from_timestamp(row.get(5), 0).unwrap_or_default(),
            updated_at: chrono::DateTime::from_timestamp(row.get(6), 0).unwrap_or_default(),
        })
    }

    // è·å–å†…å­˜ä½¿ç”¨é‡
    async fn get_memory_usage(&self) -> Result<usize> {
        // ç®€åŒ–çš„å†…å­˜ä½¿ç”¨è®¡ç®—
        Ok(0)
    }

    // æ¸…ç†è¿‡æœŸç¼“å­˜
    pub async fn cleanup_expired_cache(&self) -> Result<usize> {
        let result = sqlx::query("DELETE FROM query_cache WHERE expires_at < ?")
            .bind(Utc::now().timestamp())
            .execute(self.db.knowledge_pool())
            .await?;

        Ok(result.rows_affected() as usize)
    }
}