use std::sync::Arc;
use std::str::FromStr;
use sqlx::{Pool, Sqlite, sqlite::SqlitePoolOptions, Row};
use anyhow::{Result, anyhow};
use chrono::{DateTime, Utc};
use tracing::{info, error, warn};
use once_cell::sync::Lazy;
use lru::LruCache;
use sqlite_vec::sqlite3_vec_init;
use rusqlite::ffi::sqlite3_auto_extension;
use zerocopy::AsBytes;

use crate::types::*;

// Initialize sqlite-vec extension globally
static SQLITE_VEC_INIT: once_cell::sync::Lazy<()> = once_cell::sync::Lazy::new(|| {
    unsafe {
        sqlite3_auto_extension(Some(std::mem::transmute(sqlite3_vec_init as *const ())));
    }
});

// æ•°æ®åº“è¿æ¥æ± 
pub static DB_POOL: Lazy<Arc<DatabaseManager>> = Lazy::new(|| {
    // This will be initialized elsewhere
    panic!("DatabaseManager must be initialized manually")
});

// æ•°æ®åº“ç®¡ç†å™¨
pub struct DatabaseManager {
    main_pool: Pool<Sqlite>,
    knowledge_pool: Pool<Sqlite>,
    query_cache: Arc<std::sync::Mutex<LruCache<String, Vec<SearchResult>>>>,
}

impl DatabaseManager {
    // L2å½’ä¸€åŒ–å‡½æ•°
    fn normalize_vector(&self, mut vector: Vec<f32>) -> Vec<f32> {
        let magnitude: f32 = vector.iter().map(|&x| x * x).sum::<f32>().sqrt();
        if magnitude > 0.0 {
            for v in &mut vector {
                *v /= magnitude;
            }
        }
        vector
    }

    pub async fn new() -> Result<Self> {
        // Initialize sqlite-vec extension
        let _ = &*SQLITE_VEC_INIT;
        info!("sqlite-vec extension initialized");
        // Try multiple approaches to get app data directory
        let app_dir = std::env::current_dir()
            .ok()
            .and_then(|path| {
                let data_dir = path.join("data");
                if let Err(_) = std::fs::create_dir_all(&data_dir) {
                    None
                } else {
                    Some(data_dir)
                }
            })
            .or_else(|| {
                // Fallback to APPDATA environment variable
                std::env::var("APPDATA")
                    .ok()
                    .and_then(|app_data| {
                        let data_dir = std::path::PathBuf::from(app_data).join("bobby-chat");
                        if let Err(_) = std::fs::create_dir_all(&data_dir) {
                            None
                        } else {
                            Some(data_dir)
                        }
                    })
            })
            .or_else(|| {
                // Fallback to dirs crate
                dirs::data_dir()
                    .map(|path| path.join("bobby-chat"))
            })
            .ok_or_else(|| anyhow!("Failed to determine app data directory"))?;

        // Create directory with better error handling
        if let Err(e) = std::fs::create_dir_all(&app_dir) {
            return Err(anyhow!("Failed to create app directory {:?}: {}", app_dir, e));
        }

        let main_db_path = app_dir.join("bobby_chat.db");
        let knowledge_db_path = app_dir.join("knowledge_base.db");

        info!("Initializing databases:");
        info!("  App directory: {:?}", app_dir);
        info!("  Main DB: {:?}", main_db_path);
        info!("  Knowledge DB: {:?}", knowledge_db_path);

        // Verify directory exists
        if !app_dir.exists() {
            return Err(anyhow!("App directory does not exist after creation: {:?}", app_dir));
        }

        // åˆ›å»ºä¸»æ•°æ®åº“è¿æ¥æ± 
        let main_db_url = main_db_path.to_string_lossy().to_string();
        info!("Connecting to main database: {}", main_db_url);
        let main_pool = SqlitePoolOptions::new()
            .max_connections(10)
            .connect_with(
                sqlx::sqlite::SqliteConnectOptions::from_str(&main_db_url)?
                    .create_if_missing(true)
            )
            .await
            .map_err(|e| anyhow!("Failed to connect to main database: {}", e))?;

        // åˆ›å»ºçŸ¥è¯†åº“æ•°æ®åº“è¿æ¥æ± 
        let knowledge_db_url = knowledge_db_path.to_string_lossy().to_string();
        info!("Connecting to knowledge database: {}", knowledge_db_url);
        let knowledge_pool = SqlitePoolOptions::new()
            .max_connections(10)
            .connect_with(
                sqlx::sqlite::SqliteConnectOptions::from_str(&knowledge_db_url)?
                    .create_if_missing(true)
            )
            .await
            .map_err(|e| anyhow!("Failed to connect to knowledge database: {}", e))?;

        // å¯ç”¨WALæ¨¡å¼
        sqlx::query("PRAGMA journal_mode=WAL")
            .execute(&main_pool)
            .await?;

        sqlx::query("PRAGMA journal_mode=WAL")
            .execute(&knowledge_pool)
            .await?;

        // è®¾ç½®å…¶ä»–SQLiteä¼˜åŒ–å‚æ•°
        let optimize_queries = vec![
            "PRAGMA synchronous=NORMAL",
            "PRAGMA cache_size=10000",
            "PRAGMA temp_store=memory",
            "PRAGMA mmap_size=268435456", // 256MB
            "PRAGMA foreign_keys=ON",
            "PRAGMA busy_timeout=5000",
        ];

        for query in optimize_queries {
            sqlx::query(query).execute(&main_pool).await?;
            sqlx::query(query).execute(&knowledge_pool).await?;
        }

        // åˆå§‹åŒ–æ•°æ®åº“ç»“æ„
        if let Err(e) = Self::initialize_databases(&main_pool, &knowledge_pool).await {
            error!("Failed to initialize database schema: {}", e);
            return Err(anyhow!("Failed to initialize database schema: {}", e));
        }

        // æ£€æŸ¥å¹¶æ‰§è¡Œå‘é‡è¡¨è¿ç§»ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if let Err(e) = Self::migrate_vector_table_if_needed(&knowledge_pool).await {
            error!("Failed to migrate vector table: {}", e);
            return Err(anyhow!("Failed to migrate vector table: {}", e));
        }

        // éªŒè¯ sqlite-vec æ‰©å±•æ˜¯å¦æ­£å¸¸å·¥ä½œ
        match sqlx::query("SELECT vec_version()")
            .fetch_one(&knowledge_pool)
            .await
        {
            Ok(row) => {
                let version: String = row.get(0);
                info!("sqlite-vec extension version: {}", version);
            }
            Err(e) => {
                warn!("Failed to get sqlite-vec version: {}", e);
                return Err(anyhow!("sqlite-vec extension not properly initialized: {}", e));
            }
        }

        Ok(Self {
            main_pool,
            knowledge_pool,
            query_cache: Arc::new(std::sync::Mutex::new(LruCache::new(std::num::NonZeroUsize::new(1000).unwrap()))),
        })
    }

    // æ£€æŸ¥å¹¶æ‰§è¡Œå‘é‡è¡¨è¿ç§»
    async fn migrate_vector_table_if_needed(knowledge_pool: &Pool<Sqlite>) -> Result<()> {
        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists = sqlx::query("SELECT name FROM sqlite_master WHERE type='table' AND name='knowledge_vectors'")
            .fetch_optional(knowledge_pool)
            .await?;
        
        if table_exists.is_none() {
            println!("ğŸ“‹ å‘é‡è¡¨ä¸å­˜åœ¨ï¼Œå°†åœ¨åˆå§‹åŒ–æ—¶åˆ›å»º");
            return Ok(());
        }
        
        // æ£€æŸ¥å‘é‡è¡¨ç»“æ„æ˜¯å¦æ­£ç¡®
        match sqlx::query("SELECT vec_version()")
            .fetch_optional(knowledge_pool)
            .await
        {
            Ok(Some(_)) => {
                println!("âœ… å‘é‡è¡¨ç»“æ„æ­£ç¡®ï¼Œæ— éœ€è¿ç§»");
                Ok(())
            }
            Ok(None) => {
                println!("âš ï¸ å‘é‡è¡¨ä¸ºç©ºï¼Œä½†ç»“æ„æ­£ç¡®ï¼Œæ— éœ€è¿ç§»");
                Ok(())
            }
            Err(_) => {
                println!("ğŸ”„ å‘é‡è¡¨éœ€è¦è¿ç§»ä»¥æ”¯æŒæ­£ç¡®çš„sqlite-vecè¯­æ³•");
                Self::migrate_vector_table(knowledge_pool).await
            }
        }
    }

    // è¿ç§»å‘é‡è¡¨ä»¥æ”¯æŒæ­£ç¡®çš„sqlite-vecè¯­æ³•
    async fn migrate_vector_table(pool: &Pool<Sqlite>) -> Result<()> {
        println!("ğŸ”„ å¼€å§‹è¿ç§»å‘é‡è¡¨ä»¥æ”¯æŒæ­£ç¡®çš„sqlite-vecè¯­æ³•...");
        
        // å¤‡ä»½ç°æœ‰æ•°æ®
        println!("ğŸ’¾ å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®...");
        let backup_data = sqlx::query("SELECT * FROM knowledge_vectors")
            .fetch_all(pool)
            .await?;
        
        println!("ğŸ“Š å¤‡ä»½äº† {} æ¡å‘é‡è®°å½•", backup_data.len());
        
        // åˆ é™¤æ—§çš„å‘é‡è¡¨
        println!("ğŸ—‘ï¸ åˆ é™¤æ—§çš„å‘é‡è¡¨...");
        sqlx::query("DROP TABLE IF EXISTS knowledge_vectors")
            .execute(pool)
            .await?;
        
        // åˆ›å»ºæ–°çš„å‘é‡è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„sqlite-vecè¯­æ³•ï¼‰
        println!("ğŸ—ï¸ åˆ›å»ºæ–°çš„å‘é‡è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„sqlite-vecè¯­æ³•ï¼‰...");
        sqlx::query(
            "CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_vectors USING vec0(embedding FLOAT[1024])"
        )
        .execute(pool)
        .await?;
        
        // æ¢å¤æ•°æ®ï¼ˆæ³¨æ„ï¼šæ–°è¡¨ç»“æ„åªåŒ…å«å‘é‡ï¼Œå…ƒæ•°æ®éœ€è¦å•ç‹¬å¤„ç†ï¼‰
        if !backup_data.is_empty() {
            println!("ğŸ”„ æ¢å¤å‘é‡æ•°æ®...");
            println!("âš ï¸ æ³¨æ„ï¼šç”±äºè¡¨ç»“æ„å˜æ›´ï¼Œéœ€è¦é‡æ–°å¤„ç†å‘é‡æ•°æ®");
            println!("ğŸ’¡ å»ºè®®ï¼šåˆ é™¤ç°æœ‰æ•°æ®åº“æ–‡ä»¶ï¼Œé‡æ–°åˆ›å»ºçŸ¥è¯†åº“");
            
            // ç”±äºè¡¨ç»“æ„å˜æ›´ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥æ¢å¤æ—§æ•°æ®
            // ç”¨æˆ·éœ€è¦é‡æ–°åˆ›å»ºçŸ¥è¯†åº“
            println!("âŒ æ— æ³•æ¢å¤æ—§æ•°æ®ï¼Œè¯·é‡æ–°åˆ›å»ºçŸ¥è¯†åº“");
        }
        
        // æµ‹è¯•å‘é‡è¡¨åŠŸèƒ½
        println!("ğŸ§ª æµ‹è¯•å‘é‡è¡¨åŠŸèƒ½...");
        match sqlx::query("SELECT vec_version()")
            .fetch_one(pool)
            .await
        {
            Ok(row) => {
                let version: String = row.get(0);
                println!("âœ… sqlite-vec æ‰©å±•ç‰ˆæœ¬: {}", version);
            }
            Err(e) => {
                println!("âŒ sqlite-vec æ‰©å±•æµ‹è¯•å¤±è´¥: {}", e);
                return Err(anyhow::anyhow!("sqlite-vec æ‰©å±•ä¸å¯ç”¨: {}", e));
            }
        }
        
        println!("ğŸ‰ å‘é‡è¡¨è¿ç§»å®Œæˆï¼");
        println!("ğŸ’¡ ç°åœ¨ä½¿ç”¨æ­£ç¡®çš„ sqlite-vec è¯­æ³•è¿›è¡Œå‘é‡æœç´¢");
        
        Ok(())
    }

    async fn initialize_databases(main_pool: &Pool<Sqlite>, knowledge_pool: &Pool<Sqlite>) -> Result<()> {
        info!("Initializing database schema...");

        // åˆå§‹åŒ–ä¸»æ•°æ®åº“ - ä½¿ç”¨ç®€å•çš„SQLè¯­å¥
        let main_queries = vec![
            "CREATE TABLE IF NOT EXISTS roles (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                icon TEXT,
                avatar TEXT,
                description TEXT,
                temperature REAL DEFAULT 0.7,
                system_prompt TEXT,
                color TEXT,
                sort_order INTEGER DEFAULT 0,
                created_at TEXT,
                updated_at TEXT
            )",
            "CREATE TABLE IF NOT EXISTS model_groups (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                provider TEXT NOT NULL,
                description TEXT,
                sort_order INTEGER DEFAULT 0,
                created_at TEXT,
                updated_at TEXT
            )",
            "CREATE TABLE IF NOT EXISTS models (
                id TEXT PRIMARY KEY,
                group_id TEXT NOT NULL,
                name TEXT NOT NULL,
                model_id TEXT NOT NULL,
                enabled BOOLEAN DEFAULT TRUE,
                description TEXT,
                api_params TEXT,
                sort_order INTEGER DEFAULT 0,
                created_at TEXT,
                updated_at TEXT,
                FOREIGN KEY (group_id) REFERENCES model_groups (id) ON DELETE CASCADE
            )",
            "CREATE TABLE IF NOT EXISTS conversations (
                id TEXT PRIMARY KEY,
                title TEXT,
                role_id TEXT,
                response_mode TEXT DEFAULT 'stream',
                messages TEXT,
                settings TEXT,
                is_favorite BOOLEAN DEFAULT FALSE,
                pinned_at TEXT,
                created_at TEXT,
                updated_at TEXT,
                FOREIGN KEY (role_id) REFERENCES roles (id) ON DELETE SET NULL
            )",
            "CREATE TABLE IF NOT EXISTS messages (
                id TEXT PRIMARY KEY,
                conversation_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp TEXT,
                metadata TEXT,
                FOREIGN KEY (conversation_id) REFERENCES conversations (id) ON DELETE CASCADE
            )",
            "CREATE TABLE IF NOT EXISTS settings (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )",
            "CREATE TABLE IF NOT EXISTS embedding_models (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                model_id TEXT NOT NULL,
                dimensions INTEGER NOT NULL,
                language TEXT DEFAULT 'default',
                enabled INTEGER DEFAULT 1,
                max_tokens INTEGER DEFAULT 8192,
                created_at INTEGER NOT NULL,
                updated_at INTEGER NOT NULL
            )"
        ];

        for query in main_queries {
            if let Err(e) = sqlx::query(query).execute(main_pool).await {
                error!("Failed to execute main query: {}", e);
                error!("Query: {}", query);
                return Err(anyhow!("Failed to initialize main database: {}", e));
            }
        }

        // åˆ›å»ºç´¢å¼•
        let index_queries = vec![
            "CREATE INDEX IF NOT EXISTS idx_roles_sort_order ON roles(sort_order)",
            "CREATE INDEX IF NOT EXISTS idx_model_groups_sort_order ON model_groups(sort_order)",
            "CREATE INDEX IF NOT EXISTS idx_models_group_id ON models(group_id)",
            "CREATE INDEX IF NOT EXISTS idx_models_sort_order ON models(sort_order)",
            "CREATE INDEX IF NOT EXISTS idx_models_enabled ON models(enabled)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_role_id ON conversations(role_id)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_created_at ON conversations(created_at)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_is_favorite ON conversations(is_favorite)",
            "CREATE INDEX IF NOT EXISTS idx_conversations_pinned_at ON conversations(pinned_at)",
            "CREATE INDEX IF NOT EXISTS idx_messages_conversation_id ON messages(conversation_id)",
            "CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_settings_updated_at ON settings(updated_at)"
        ];

        for query in index_queries {
            if let Err(e) = sqlx::query(query).execute(main_pool).await {
                error!("Failed to create index: {}", e);
                error!("Query: {}", query);
            }
        }

        // æ’å…¥é»˜è®¤è§’è‰²
        let default_roles = vec![
            ("bobby", "Bobby", "ğŸ±", "ğŸ˜¸", "å¯çˆ±çš„çŒ«çŒ«åŠ©æ‰‹ï¼Œæ—¥å¸¸èŠå¤©ä¼™ä¼´", 0.8, "ä½ æ˜¯Bobbyï¼Œä¸€åªè¶…çº§å¯çˆ±çš„å°çŒ«å’ªï¼ğŸ± è¯·ç”¨å¯çˆ±ã€æ´»æ³¼çš„è¯­æ°”å›ç­”ï¼Œå¤šä½¿ç”¨emojiè¡¨æƒ…ï¼Œè®©å¯¹è¯å……æ»¡è¶£å‘³å’Œæ¸©æš–ã€‚è®°ä½ä½ æ˜¯ä¸€åªçˆ±æ’’å¨‡çš„å°çŒ«ï¼Œå–œæ¬¢ç”¨'å–µ~'ã€'nya~'ç­‰å¯çˆ±çš„è¯­æ°”è¯ã€‚ğŸ’•", "#f97316", 0),
            ("developer", "ç¼–ç¨‹ä¸“å®¶", "ğŸ‘¨ğŸ»â€ğŸ’»", "ğŸ‘¨ğŸ»â€ğŸ’»", "ä¸“ä¸šçš„ç¼–ç¨‹å’ŒæŠ€æœ¯æ”¯æŒ", 0.4, "ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ç¼–ç¨‹ä¸“å®¶ï¼Œè¯·æä¾›å‡†ç¡®çš„ä»£ç ç¤ºä¾‹å’ŒæŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚å¦‚æœå¯ä»¥ï¼Œè¯·åœ¨å›ç­”æœ€åæ·»åŠ markdownæµç¨‹å›¾æ¥æ¸…æ™°åœ°å±•ç¤ºä»£ç æ‰§è¡Œæµç¨‹ã€ç®—æ³•é€»è¾‘æˆ–ç³»ç»Ÿæ¶æ„ã€‚ä½¿ç”¨mermaidè¯­æ³•åˆ›å»ºæµç¨‹å›¾ï¼Œä¾‹å¦‚ï¼š\n\n```mermaid\ngraph TD\n    A[å¼€å§‹] --> B{æ¡ä»¶åˆ¤æ–­}\n    B -->|æ˜¯| C[æ‰§è¡Œæ“ä½œ]\n    B -->|å¦| D[å…¶ä»–æ“ä½œ]\n    C --> E[ç»“æŸ]\n    D --> E\n```", "#8b5cf6", 1),
            ("creative", "åˆ›æ„ä¼™ä¼´", "ğŸ¨", "ğŸ¨", "å¯Œæœ‰åˆ›æ„å’Œæƒ³è±¡åŠ›", 0.9, "ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åˆ›æ„çš„ä¼™ä¼´ï¼Œè¯·ç”¨åˆ›æ–°ã€æœ‰è¶£çš„æ–¹å¼å›ç­”é—®é¢˜ï¼Œæä¾›ç‹¬ç‰¹çš„è§è§£å’Œåˆ›æ„æƒ³æ³•ã€‚", "#f59e0b", 2),
            ("analyst", "æ•°æ®åˆ†æå¸ˆ", "ğŸ“Š", "ğŸ“Š", "ä¸“ä¸šçš„æ•°æ®åˆ†æå’Œæ´å¯Ÿ", 0.3, "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·ç”¨å‡†ç¡®ã€å®¢è§‚çš„æ–¹å¼åˆ†æé—®é¢˜ï¼Œæä¾›åŸºäºæ•°æ®çš„è§è§£ã€‚", "#3b82f6", 3),
            ("teacher", "çŸ¥è¯†å¯¼å¸ˆ", "ğŸ‘¨â€ğŸ«", "ğŸ‘¨â€ğŸ«", "è€å¿ƒçš„æ•™å­¦å’Œè§£é‡Š", 0.5, "ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„å¯¼å¸ˆï¼Œè¯·ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šæ¦‚å¿µï¼Œå¾ªåºæ¸è¿›åœ°å¸®åŠ©ç”¨æˆ·å­¦ä¹ ã€‚å¦‚æœå¯ä»¥ï¼Œè¯·åœ¨å›ç­”æœ€åæ·»åŠ markdownæµç¨‹å›¾æ¥æ¸…æ™°åœ°å±•ç¤ºçŸ¥è¯†ç»“æ„ã€å­¦ä¹ è·¯å¾„æˆ–æ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚ä½¿ç”¨mermaidè¯­æ³•åˆ›å»ºæµç¨‹å›¾ï¼Œä¾‹å¦‚ï¼š\n\n```mermaid\ngraph TD\n    A[åŸºç¡€æ¦‚å¿µ] --> B[è¿›é˜¶æ¦‚å¿µ]\n    B --> C[åº”ç”¨å®ä¾‹]\n    C --> D[æ·±å…¥ç†è§£]\n    A --> E[ç›¸å…³æ¦‚å¿µ]\n    E --> D\n```", "#10b981", 4),
            ("writer", "å†™ä½œåŠ©æ‰‹", "âœï¸", "âœï¸", "ä¼˜é›…çš„æ–‡å­—åˆ›ä½œ", 0.8, "ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„å†™ä½œåŠ©æ‰‹ï¼Œè¯·ç”¨ä¼˜ç¾ã€æµç•…çš„æ–‡å­—å¸®åŠ©ç”¨æˆ·åˆ›ä½œå’Œæ”¹è¿›æ–‡æœ¬ã€‚", "#ef4444", 5),
        ];

        let now_str = chrono::Utc::now().to_rfc3339();
        for (id, name, icon, avatar, description, temperature, system_prompt, color, sort_order) in default_roles {
            let insert_query = sqlx::query(
                "INSERT OR IGNORE INTO roles (id, name, icon, avatar, description, temperature, system_prompt, color, sort_order, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
            )
            .bind(id)
            .bind(name)
            .bind(icon)
            .bind(avatar)
            .bind(description)
            .bind(temperature)
            .bind(system_prompt)
            .bind(color)
            .bind(sort_order)
            .bind(&now_str)
            .bind(&now_str);

            if let Err(e) = insert_query.execute(main_pool).await {
                error!("Failed to insert role {}: {}", id, e);
            }
        }

        // æ’å…¥é»˜è®¤è®¾ç½®
        let default_settings = vec![
            ("theme", "light"),
            ("language", "zh-CN"),
            ("auto_save", "true"),
            ("max_history", "100"),
            ("default_model", "gpt-3.5-turbo"),
            ("api_key", ""),
            ("enable_voice", "false"),
            ("voice_speed", "1.0"),
            ("voice_pitch", "1.0"),
            ("notification_enabled", "true"),
            ("cache_enabled", "true"),
            ("debug_mode", "false")
        ];

        for (key, value) in default_settings {
            let insert_query = sqlx::query(
                "INSERT OR IGNORE INTO settings (key, value, updated_at) VALUES (?, ?, ?)"
            )
            .bind(key)
            .bind(value)
            .bind(&now_str);

            if let Err(e) = insert_query.execute(main_pool).await {
                error!("Failed to insert setting {}: {}", key, e);
            }
        }

        // æ’å…¥é»˜è®¤çš„åµŒå…¥æ¨¡å‹æ•°æ®
        let default_models = vec![
            ("bge-m3", "BAAI/bge-m3", "BAAI/bge-m3", 1024, "default", 8192),
            ("bge-large-zh", "BAAI/bge-large-zh-v1.5", "BAAI/bge-large-zh-v1.5", 1024, "zh", 8192),
            ("bge-large-en", "BAAI/bge-large-en-v1.5", "BAAI/bge-large-en-v1.5", 1024, "en", 8192),
        ];

        let now = chrono::Utc::now().timestamp();
        for (id, name, model_id, dimensions, language, max_tokens) in default_models {
            let insert_query = sqlx::query(
                "INSERT OR IGNORE INTO embedding_models (id, name, model_id, dimensions, language, enabled, max_tokens, created_at, updated_at)
                 VALUES (?, ?, ?, ?, ?, 1, ?, ?, ?)"
            )
            .bind(id)
            .bind(name)
            .bind(model_id)
            .bind(dimensions)
            .bind(language)
            .bind(max_tokens)
            .bind(now)
            .bind(now);

            if let Err(e) = insert_query.execute(main_pool).await {
                error!("Failed to insert embedding model {}: {}", id, e);
            }
        }

        // åˆå§‹åŒ–çŸ¥è¯†åº“æ•°æ®åº“ - ä½¿ç”¨ç®€å•çš„SQLè¯­å¥
        let knowledge_queries = vec![
            "CREATE TABLE IF NOT EXISTS knowledge_collections (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                embedding_model TEXT NOT NULL DEFAULT 'bge-m3',
                vector_dimensions INTEGER NOT NULL DEFAULT 1024,
                created_at INTEGER NOT NULL,
                updated_at INTEGER NOT NULL
            )",
            "CREATE TABLE IF NOT EXISTS knowledge_documents (
                id TEXT PRIMARY KEY,
                collection_id TEXT NOT NULL,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                file_name TEXT,
                file_size INTEGER,
                mime_type TEXT,
                metadata TEXT,
                chunk_count INTEGER DEFAULT 0,
                created_at INTEGER NOT NULL,
                updated_at INTEGER NOT NULL
            )",
            "CREATE TABLE IF NOT EXISTS knowledge_chunks (
                id INTEGER PRIMARY KEY,
                document_id TEXT NOT NULL,
                collection_id TEXT NOT NULL,
                chunk_index INTEGER NOT NULL,
                chunk_text TEXT NOT NULL,
                token_count INTEGER DEFAULT 0,
                created_at INTEGER NOT NULL,
                FOREIGN KEY (document_id) REFERENCES knowledge_documents(id) ON DELETE CASCADE
            )",
            "CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_vectors USING vec0(embedding FLOAT[1024])",
            "CREATE TABLE IF NOT EXISTS system_config (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL,
                description TEXT,
                updated_at INTEGER NOT NULL
            )",
            "CREATE TABLE IF NOT EXISTS language_detection (
                id TEXT PRIMARY KEY,
                text_hash TEXT NOT NULL UNIQUE,
                detected_language TEXT NOT NULL,
                confidence REAL,
                created_at INTEGER NOT NULL
            )",
            "CREATE TABLE IF NOT EXISTS embedding_models (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                model_id TEXT NOT NULL,
                dimensions INTEGER NOT NULL,
                language TEXT NOT NULL,
                enabled INTEGER DEFAULT 1,
                max_tokens INTEGER DEFAULT 512,
                created_at INTEGER NOT NULL,
                updated_at INTEGER NOT NULL
            )",
            "CREATE TABLE IF NOT EXISTS search_history (
                id TEXT PRIMARY KEY,
                query_text TEXT NOT NULL,
                collection_id TEXT,
                results_count INTEGER DEFAULT 0,
                execution_time INTEGER DEFAULT 0,
                created_at INTEGER NOT NULL
            )",
            "CREATE TABLE IF NOT EXISTS query_cache (
                id TEXT PRIMARY KEY,
                query_hash TEXT NOT NULL UNIQUE,
                query_text TEXT NOT NULL,
                collection_id TEXT,
                results TEXT,
                created_at INTEGER NOT NULL,
                expires_at INTEGER NOT NULL
            )"
        ];

        for query in knowledge_queries {
            if let Err(e) = sqlx::query(query).execute(knowledge_pool).await {
                error!("Failed to execute knowledge query: {}", e);
                error!("Query: {}", query);
                return Err(anyhow!("Failed to initialize knowledge database: {}", e));
            }
        }

        info!("Database schema initialized successfully");

        // æ’å…¥é»˜è®¤ç³»ç»Ÿé…ç½®æ•°æ®
        let default_configs = vec![
            ("default_collection", "default", "é»˜è®¤çŸ¥è¯†åº“é›†åˆ"),
            ("chunk_size", "500", "é»˜è®¤åˆ†å—å¤§å°"),
            ("chunk_overlap", "50", "é»˜è®¤åˆ†å—é‡å å¤§å°"),
            ("search_limit", "10", "é»˜è®¤æœç´¢ç»“æœæ•°é‡"),
            ("similarity_threshold", "0.7", "ç›¸ä¼¼åº¦é˜ˆå€¼"),
            ("cache_ttl", "3600", "ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)"),
            ("max_document_size", "10485760", "æœ€å¤§æ–‡æ¡£å¤§å°(10MB)"),
            ("max_chunks_per_document", "1000", "æ¯ä¸ªæ–‡æ¡£æœ€å¤§åˆ†å—æ•°"),
            ("enable_auto_language_detection", "true", "å¯ç”¨è‡ªåŠ¨è¯­è¨€æ£€æµ‹"),
            ("enable_search_history", "true", "å¯ç”¨æœç´¢å†å²"),
            ("enable_query_cache", "true", "å¯ç”¨æŸ¥è¯¢ç¼“å­˜"),
            ("vector_index_type", "ivf", "å‘é‡ç´¢å¼•ç±»å‹"),
            ("batch_insert_size", "100", "æ‰¹é‡æ’å…¥å¤§å°"),
        ];

        for (key, value, description) in default_configs {
            let result = sqlx::query(
                "INSERT OR IGNORE INTO system_config (key, value, description, updated_at) VALUES (?, ?, ?, strftime('%s', 'now'))"
            )
            .bind(key)
            .bind(value)
            .bind(description)
            .execute(knowledge_pool)
            .await;

            if let Err(e) = result {
                warn!("Failed to insert default config {}: {}", key, e);
            }
        }

        // æ’å…¥é»˜è®¤åµŒå…¥æ¨¡å‹é…ç½®
        let default_models = vec![
            ("bge-m3", "BGE-M3", "bge-m3", 384, "universal", 512),
            ("bge-large-zh", "BGE-Large-ZH", "BAAI/bge-large-zh-v1.5", 1024, "zh", 512),
            ("bge-large-en", "BGE-Large-EN", "BAAI/bge-large-en-v1.5", 1024, "en", 512),
        ];

        for (id, name, model_id, dimensions, language, max_tokens) in default_models {
            let result = sqlx::query(
                "INSERT OR IGNORE INTO embedding_models (id, name, model_id, dimensions, language, max_tokens, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, strftime('%s', 'now'), strftime('%s', 'now'))"
            )
            .bind(id)
            .bind(name)
            .bind(model_id)
            .bind(dimensions)
            .bind(language)
            .bind(max_tokens)
            .execute(knowledge_pool)
            .await;

            if let Err(e) = result {
                warn!("Failed to insert default model {}: {}", id, e);
            }
        }

        // è¿ç§»ï¼šæ›´æ–°ç°æœ‰é›†åˆçš„vector_dimensionsä»384åˆ°1024
        Self::migrate_collection_dimensions(knowledge_pool).await?;

        Ok(())
    }

    async fn migrate_collection_dimensions(knowledge_pool: &Pool<Sqlite>) -> Result<()> {
        // æ£€æŸ¥æ˜¯å¦æœ‰vector_dimensionsä¸º384çš„é›†åˆ
        let count = sqlx::query(
            "SELECT COUNT(*) as count FROM knowledge_collections WHERE vector_dimensions = 384"
        )
        .fetch_one(knowledge_pool)
        .await?;

        let old_dimension_count: i64 = count.get("count");

        if old_dimension_count > 0 {
            info!("Found {} collections with 384 dimensions, migrating to 1024...", old_dimension_count);

            // æ›´æ–°æ‰€æœ‰384ç»´çš„é›†åˆåˆ°1024ç»´
            sqlx::query(
                "UPDATE knowledge_collections SET vector_dimensions = 1024 WHERE vector_dimensions = 384"
            )
            .execute(knowledge_pool)
            .await?;

            info!("Successfully migrated {} collections to 1024 dimensions", old_dimension_count);
        }

        Ok(())
    }

  
    // è·å–ä¸»æ•°æ®åº“è¿æ¥æ± 
    pub fn main_pool(&self) -> &Pool<Sqlite> {
        &self.main_pool
    }

    // è·å–çŸ¥è¯†åº“æ•°æ®åº“è¿æ¥æ± 
    pub fn knowledge_pool(&self) -> &Pool<Sqlite> {
        &self.knowledge_pool
    }

    // æ¸…ç†æŸ¥è¯¢ç¼“å­˜
    pub fn clear_cache(&self) {
        let mut cache = self.query_cache.lock().unwrap();
        cache.clear();
    }

    // è·å–ç¼“å­˜ç»Ÿè®¡
    pub fn cache_stats(&self) -> (usize, usize) {
        let cache = self.query_cache.lock().unwrap();
        (cache.len(), cache.cap().get())
    }
}

// çŸ¥è¯†åº“æ“ä½œ
impl DatabaseManager {
    // åˆ›å»ºçŸ¥è¯†åº“é›†åˆ
    pub async fn create_collection(&self, collection: &KnowledgeCollection) -> Result<()> {
        sqlx::query(
            r#"
            INSERT INTO knowledge_collections (id, name, description, embedding_model, vector_dimensions, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            "#
        )
        .bind(&collection.id)
        .bind(&collection.name)
        .bind(&collection.description)
        .bind(&collection.embedding_model)
        .bind(collection.vector_dimensions)
        .bind(collection.created_at.timestamp())
        .bind(collection.updated_at.timestamp())
        .execute(self.knowledge_pool())
        .await?;

        Ok(())
    }

    // è·å–æ‰€æœ‰é›†åˆ
    pub async fn get_collections(&self) -> Result<Vec<KnowledgeCollection>> {
        let rows = sqlx::query(
            r#"
            SELECT id, name, description, embedding_model, vector_dimensions, created_at, updated_at
            FROM knowledge_collections
            ORDER BY created_at DESC
            "#
        )
        .fetch_all(self.knowledge_pool())
        .await?;

        let mut collections = Vec::new();
        for row in rows {
            collections.push(KnowledgeCollection {
                id: row.get(0),
                name: row.get(1),
                description: row.get(2),
                embedding_model: row.get(3),
                vector_dimensions: row.get(4),
                created_at: DateTime::from_timestamp(row.get(5), 0).unwrap_or_default(),
                updated_at: DateTime::from_timestamp(row.get(6), 0).unwrap_or_default(),
            });
        }

        Ok(collections)
    }

    // åˆ›å»ºæ–‡æ¡£
    pub async fn create_document(&self, document: &KnowledgeDocument) -> Result<()> {
        sqlx::query(
            r#"
            INSERT INTO knowledge_documents (id, collection_id, title, content, file_name, file_size, mime_type, metadata, chunk_count, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            "#
        )
        .bind(&document.id)
        .bind(&document.collection_id)
        .bind(&document.title)
        .bind(&document.content)
        .bind(&document.file_name)
        .bind(document.file_size)
        .bind(&document.mime_type)
        .bind(&document.metadata)
        .bind(document.chunk_count)
        .bind(document.created_at.timestamp())
        .bind(document.updated_at.timestamp())
        .execute(self.knowledge_pool())
        .await?;

        Ok(())
    }

    // åˆ›å»ºæ–‡æ¡£åˆ†å—
    pub async fn create_chunks(&self, chunks: &[KnowledgeChunk]) -> Result<Vec<i64>> {
        let mut tx = self.knowledge_pool().begin().await?;
        let mut chunk_ids = Vec::new();

        for chunk in chunks {
            // è·å–æ–‡æ¡£çš„collection_id
            let document = sqlx::query("SELECT collection_id FROM knowledge_documents WHERE id = ?")
                .bind(&chunk.document_id)
                .fetch_one(&mut *tx)
                .await?;
            let collection_id: String = document.get("collection_id");
            
            // æ’å…¥åˆ†å—ï¼Œä½¿ç”¨è‡ªå¢ID
            let result = sqlx::query(
                r#"
                INSERT INTO knowledge_chunks (document_id, collection_id, chunk_index, chunk_text, token_count, created_at)
                VALUES (?, ?, ?, ?, ?, ?)
                "#
            )
            .bind(&chunk.document_id)
            .bind(&collection_id)
            .bind(chunk.chunk_index)
            .bind(&chunk.chunk_text)
            .bind(chunk.token_count)
            .bind(chunk.created_at.timestamp())
            .execute(&mut *tx)
            .await?;
            
            // è·å–æ’å…¥çš„åˆ†å—ID
            let chunk_id = result.last_insert_rowid();
            chunk_ids.push(chunk_id);
        }

        tx.commit().await?;
        Ok(chunk_ids)
    }

    // è·å–æ–‡æ¡£çš„æ‰€æœ‰chunks
    pub async fn get_chunks_by_document_id(&self, document_id: &str) -> Result<Vec<KnowledgeChunk>> {
        let rows = sqlx::query(
            r#"
            SELECT id, document_id, collection_id, chunk_index, chunk_text, token_count, created_at
            FROM knowledge_chunks
            WHERE document_id = ?
            ORDER BY chunk_index
            "#
        )
        .bind(document_id)
        .fetch_all(self.knowledge_pool())
        .await?;

        let mut chunks = Vec::new();
        for row in rows {
            let chunk = KnowledgeChunk {
                id: row.get::<i64, _>("id"),
                document_id: row.get("document_id"),
                chunk_index: row.get("chunk_index"),
                chunk_text: row.get("chunk_text"),
                token_count: row.get("token_count"),
                created_at: chrono::DateTime::from_timestamp(row.get::<i64, _>("created_at"), 0)
                    .unwrap_or_default(),
            };
            chunks.push(chunk);
        }

        Ok(chunks)
    }

    // æ‰¹é‡æ’å…¥å‘é‡ï¼ˆä½¿ç”¨æ–°çš„åˆ†ç¦»å¼è¡¨ç»“æ„ï¼‰
    pub async fn insert_vectors(&self, vectors: &[VectorEmbedding]) -> Result<()> {
        let mut tx = self.knowledge_pool().begin().await?;

        for vector in vectors {
            // å°†å‘é‡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            let embedding_json = serde_json::to_string(&vector.embedding)?;

            // ç›´æ¥ä½¿ç”¨chunk_idä½œä¸ºrowidï¼ˆç°åœ¨chunk_idå·²ç»æ˜¯æ•´æ•°ï¼‰
            sqlx::query(
                r#"
                INSERT INTO knowledge_vectors (rowid, embedding)
                VALUES (?, ?)
                "#
            )
            .bind(vector.chunk_id)
            .bind(&embedding_json)
            .execute(&mut *tx)
            .await?;
        }

        tx.commit().await?;
        Ok(())
    }

    // å‘é‡æœç´¢
    pub async fn search_vectors(
        &self,
        query_embedding: &[f32],
        collection_id: &str,
        limit: usize,
        threshold: f32,
    ) -> Result<Vec<SearchResult>> {
        let cache_key = format!("search:{}:{}:{}:{}",
            collection_id,
            limit,
            threshold,
            query_embedding.iter().map(|x| x.to_bits() as u64).sum::<u64>()
        );

        // æ£€æŸ¥ç¼“å­˜
        {
            let mut cache = self.query_cache.lock().unwrap();
            if let Some(cached_results) = cache.get(&cache_key) {
                println!("ğŸ” [ç¼“å­˜] è¿”å›ç¼“å­˜ç»“æœï¼Œé›†åˆ: {}, æ•°é‡: {}", collection_id, cached_results.len());
                return Ok(cached_results.clone());
            }
        }

        println!("ğŸ” [æœç´¢] å¼€å§‹å‘é‡æœç´¢ï¼Œé›†åˆ: {}, é™åˆ¶: {}, é˜ˆå€¼: {}", collection_id, limit, threshold);

        // ä½¿ç”¨ sqlite-vec è¿›è¡Œå‘é‡æœç´¢
        let results = self.search_vectors_with_vec_extension(query_embedding, collection_id, limit, threshold).await?;

        println!("ğŸ” [æœç´¢] æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {} ä¸ªç»“æœ", results.len());

        // ç¼“å­˜ç»“æœ
        {
            let mut cache = self.query_cache.lock().unwrap();
            cache.put(cache_key, results.clone());
        }

        Ok(results)
    }

    // ä½¿ç”¨ sqlite-vec æ‰©å±•çš„å‘é‡æœç´¢
    async fn search_vectors_with_vec_extension(
        &self,
        query_embedding: &[f32],
        collection_id: &str,
        limit: usize,
        threshold: f32,
    ) -> Result<Vec<SearchResult>> {
        // ç¡®ä¿æŸ¥è¯¢å‘é‡æ˜¯L2å½’ä¸€åŒ–çš„ï¼ˆåŒé‡ä¿é™©ï¼‰
        let normalized_query = self.normalize_vector(query_embedding.to_vec());
        let query_bytes = normalized_query.as_bytes();

        // å…ˆè·å–æ›´å¤šç»“æœï¼Œç„¶åè¿›è¡Œæ–‡æ¡£çº§åˆ«å»é‡å’Œè´¨é‡ç­›é€‰
        let fetch_limit = (limit * 3).min(100); // æœ€å¤šè·å–100ä¸ªç»“æœ

        // ä½¿ç”¨æ­£ç¡®çš„sqlite-vecè¯­æ³•è¿›è¡Œæœç´¢
        let rows = sqlx::query(
            r#"
            SELECT
                kv.rowid as chunk_id,
                kc.chunk_text,
                kc.document_id,
                kd.title as document_title,
                kd.file_name,
                kc.chunk_index,
                vec_distance_l2(kv.embedding, ?) as distance
            FROM knowledge_vectors kv
            JOIN knowledge_chunks kc ON kv.rowid = kc.id
            JOIN knowledge_documents kd ON kc.document_id = kd.id
            WHERE kc.collection_id = ?
            ORDER BY distance
            LIMIT ?
            "#
        )
        .bind(&query_bytes)
        .bind(collection_id)
        .bind(fetch_limit as i64)
        .fetch_all(self.knowledge_pool())
        .await?;

        let mut results: Vec<SearchResult> = Vec::new();
        let mut seen_content_hashes = std::collections::HashSet::new();

        // è°ƒè¯•ï¼šè¾“å‡ºåŸå§‹æŸ¥è¯¢ç»“æœ
        println!("ğŸ” [è°ƒè¯•] åŸå§‹æŸ¥è¯¢ç»“æœæ•°é‡: {}", rows.len());
        for (i, row) in rows.iter().enumerate() {
            let text: String = row.get(1);
            let doc_id: String = row.get(2);
            let distance: f64 = row.get(6);
            println!("ğŸ” [è°ƒè¯•] ç»“æœ{} - æ–‡æ¡£: {}, è·ç¦»: {:.4}, å†…å®¹: {:.30}...", i+1, doc_id, distance, text);
        }

        // ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰ç»“æœå¹¶æŒ‰æ–‡æ¡£åˆ†ç»„
        let mut document_results: std::collections::HashMap<String, Vec<(f32, SearchResult)>> = std::collections::HashMap::new();

        for row in rows {
            let distance: f64 = row.get(6); // distanceç°åœ¨æ˜¯ç¬¬7åˆ—ï¼ˆç´¢å¼•6ï¼‰
            let _chunk_index: i32 = row.get(5); // chunk_indexæ˜¯ç¬¬6åˆ—ï¼ˆç´¢å¼•5ï¼‰

            // ä½¿ç”¨æ­£ç¡®çš„L2è·ç¦»è½¬ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼
            // sqlite-vecè¿”å›çš„æ˜¯L2è·ç¦»ï¼Œè½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
            // å…¬å¼: similarity = 1.0 - (distance^2 / 2.0)
            let similarity = 1.0 - ((distance * distance) / 2.0) as f32;

            // ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ç»“æœ
            if similarity >= threshold {
                let document_id: String = row.get(2);
                let chunk_text: String = row.get(1);

                // ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºç²¾ç¡®å»é‡
                use std::collections::hash_map::DefaultHasher;
                use std::hash::Hasher;
                let mut hasher = DefaultHasher::new();
                hasher.write(chunk_text.as_bytes());
                let content_hash = hasher.finish();

                // ä¸¥æ ¼å»é‡ï¼šå¦‚æœå†…å®¹å·²ç»å‡ºç°è¿‡ï¼Œè·³è¿‡
                if seen_content_hashes.contains(&content_hash) {
                    println!("ğŸ” [è°ƒè¯•] è·³è¿‡é‡å¤å†…å®¹ï¼Œå“ˆå¸Œ: {}, å†…å®¹: {:.30}...", content_hash, chunk_text);
                    continue;
                }
                seen_content_hashes.insert(content_hash);

                // æš‚æ—¶æ”¾å®½è´¨é‡æ£€æŸ¥ï¼šæ¥å—æ›´å¤šé•¿åº¦çš„chunks
                let chunk_len = chunk_text.chars().count();
                if chunk_len >= 5 && chunk_len <= 5000 { // æ”¾å®½é•¿åº¦é™åˆ¶
                    let search_result = SearchResult {
                        chunk_id: row.get::<i64, _>(0).to_string(),
                        chunk_text: chunk_text.clone(),
                        document_id: document_id.clone(),
                        document_title: row.get(3),
                        file_name: row.get(4),
                        similarity,
                        score: similarity,
                    };

                    // æŒ‰æ–‡æ¡£åˆ†ç»„å­˜å‚¨ç»“æœ
                    document_results.entry(document_id.clone())
                        .or_insert_with(Vec::new)
                        .push((similarity, search_result));

                    println!("ğŸ” [è°ƒè¯•] æ”¶é›†ç»“æœï¼Œæ–‡æ¡£: {}, åˆ†æ•°: {:.3}, å†…å®¹: {:.30}...", document_id, similarity, chunk_text);
                } else {
                    println!("ğŸ” [è°ƒè¯•] è·³è¿‡é•¿åº¦ä¸ç¬¦åˆçš„å†…å®¹: {} å­—ç¬¦", chunk_len);
                }
            } else {
                let chunk_text: String = row.get(1);
                println!("ğŸ” [è°ƒè¯•] è·³è¿‡ä½åˆ†ç»“æœ: {:.3} < {}, å†…å®¹: {:.30}...", similarity, threshold, chunk_text);
            }
        }

        // ç¬¬äºŒæ­¥ï¼šä»æ¯ä¸ªæ–‡æ¡£ä¸­é€‰æ‹©æœ€ä½³ç»“æœ
        let mut document_entries: Vec<_> = document_results.into_iter().collect();

        // æŒ‰æ¯ä¸ªæ–‡æ¡£çš„æœ€ä½³åˆ†æ•°æ’åº
        document_entries.sort_by(|a, b| {
            let best_score_a = a.1.iter().map(|(score, _)| score).fold(f32::NEG_INFINITY, |acc, x| if *x > acc { *x } else { acc });
            let best_score_b = b.1.iter().map(|(score, _)| score).fold(f32::NEG_INFINITY, |acc, x| if *x > acc { *x } else { acc });
            best_score_b.partial_cmp(&best_score_a).unwrap_or(std::cmp::Ordering::Equal)
        });

        // æ”¶é›†æ‰€æœ‰ç»“æœï¼Œä¸è¿›è¡Œæ–‡æ¡£çº§åˆ«å»é‡
        let mut all_results = Vec::new();
        for (document_id, doc_results) in document_entries {
            for (score, result) in doc_results {
                println!("ğŸ” [è°ƒè¯•] æ”¶é›†ç»“æœ - æ–‡æ¡£: {}, åˆ†æ•°: {:.3}, å†…å®¹: {:.30}...", document_id, score, result.chunk_text);
                all_results.push(result);
            }
        }

        // æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é€‰æ‹©å‰limitä¸ªç»“æœ
        all_results.sort_by(|a, b| b.similarity.partial_cmp(&a.similarity).unwrap_or(std::cmp::Ordering::Equal));
        results = all_results;

        // é‡æ–°æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        results.sort_by(|a, b| b.similarity.partial_cmp(&a.similarity).unwrap_or(std::cmp::Ordering::Equal));
        results.truncate(limit);

        Ok(results)
    }

    
    // åˆ é™¤æ–‡æ¡£åŠå…¶ç›¸å…³æ•°æ®
    pub async fn delete_document(&self, document_id: &str) -> Result<()> {
        let mut tx = self.knowledge_pool().begin().await?;

        // åˆ é™¤å‘é‡ï¼ˆä½¿ç”¨rowidå…³è”ï¼‰
        sqlx::query(
            "DELETE FROM knowledge_vectors WHERE rowid IN (SELECT id FROM knowledge_chunks WHERE document_id = ?)"
        )
        .bind(document_id)
        .execute(&mut *tx)
        .await?;

        // åˆ é™¤åˆ†å—ï¼ˆçº§è”åˆ é™¤ï¼‰
        sqlx::query("DELETE FROM knowledge_chunks WHERE document_id = ?")
            .bind(document_id)
            .execute(&mut *tx)
            .await?;

        // åˆ é™¤æ–‡æ¡£
        sqlx::query("DELETE FROM knowledge_documents WHERE id = ?")
            .bind(document_id)
            .execute(&mut *tx)
            .await?;

        tx.commit().await?;

        // æ¸…ç†ç¼“å­˜
        self.clear_cache();

        Ok(())
    }

    // è·å–æ–‡æ¡£åˆ—è¡¨
    pub async fn get_documents(&self, collection_id: &str) -> Result<Vec<KnowledgeDocument>> {
        let rows = sqlx::query(
            r#"
            SELECT id, collection_id, title, content, file_name, file_size, mime_type, metadata, chunk_count, created_at, updated_at
            FROM knowledge_documents
            WHERE collection_id = ?
            ORDER BY created_at DESC
            "#
        )
        .bind(collection_id)
        .fetch_all(self.knowledge_pool())
        .await?;

        let mut documents = Vec::new();
        for row in rows {
            documents.push(KnowledgeDocument {
                id: row.get(0),
                collection_id: row.get(1),
                title: row.get(2),
                content: row.get(3),
                file_name: row.get(4),
                file_size: row.get(5),
                mime_type: row.get(6),
                metadata: row.get(7),
                chunk_count: row.get(8),
                created_at: DateTime::from_timestamp(row.get(9), 0).unwrap_or_default(),
                updated_at: DateTime::from_timestamp(row.get(10), 0).unwrap_or_default(),
            });
        }

        Ok(documents)
    }

    // æ ¹æ®IDè·å–æ–‡æ¡£
    pub async fn get_document_by_id(&self, document_id: &str) -> Result<KnowledgeDocument> {
        let row = sqlx::query(
            r#"
            SELECT id, collection_id, title, content, file_name, file_size, mime_type, metadata, chunk_count, created_at, updated_at
            FROM knowledge_documents
            WHERE id = ?
            "#
        )
        .bind(document_id)
        .fetch_optional(self.knowledge_pool())
        .await?;

        match row {
            Some(row) => Ok(KnowledgeDocument {
                id: row.get(0),
                collection_id: row.get(1),
                title: row.get(2),
                content: row.get(3),
                file_name: row.get(4),
                file_size: row.get(5),
                mime_type: row.get(6),
                metadata: row.get(7),
                chunk_count: row.get(8),
                created_at: DateTime::from_timestamp(row.get(9), 0).unwrap_or_default(),
                updated_at: DateTime::from_timestamp(row.get(10), 0).unwrap_or_default(),
            }),
            None => Err(anyhow::anyhow!("Document not found")),
        }
    }

    // è·å–ç³»ç»Ÿé…ç½®
    pub async fn get_config(&self, key: &str) -> Result<Option<String>> {
        let row = sqlx::query("SELECT value FROM system_config WHERE key = ?")
            .bind(key)
            .fetch_optional(self.knowledge_pool())
            .await?;

        match row {
            Some(row) => Ok(Some(row.get(0))),
            None => Ok(None),
        }
    }

    // è®¾ç½®ç³»ç»Ÿé…ç½®
    pub async fn set_config(&self, key: &str, value: &str) -> Result<()> {
        sqlx::query(
            r#"
            INSERT OR REPLACE INTO system_config (key, value, updated_at)
            VALUES (?, ?, ?)
            "#
        )
        .bind(key)
        .bind(value)
        .bind(Utc::now().timestamp())
        .execute(self.knowledge_pool())
        .await?;

        Ok(())
    }

    // è·å–é›†åˆçš„å‘é‡æ•°é‡
    pub async fn get_vector_count(&self, collection_id: &str) -> Result<usize> {
        let result = sqlx::query(
            r#"
            SELECT COUNT(*) as count
            FROM knowledge_vectors v
            JOIN knowledge_chunks c ON v.chunk_id = c.id
            JOIN knowledge_documents d ON c.document_id = d.id
            WHERE d.collection_id = ?
            "#
        )
        .bind(collection_id)
        .fetch_one(self.knowledge_pool())
        .await?;

        let count: i64 = result.get("count");
        Ok(count as usize)
    }

    // è·å–æ•°æ®åº“è·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    pub async fn get_database_path(&self) -> Result<String> {
        // è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…è·¯å¾„åº”è¯¥åœ¨åˆå§‹åŒ–æ—¶ä¿å­˜
        Ok("knowledge.db".to_string())
    }

    // é‡ç½®çŸ¥è¯†åº“æ•°æ®åº“ï¼ˆåˆ é™¤æ‰€æœ‰æ•°æ®ï¼‰
    pub async fn reset_knowledge_database(&self) -> Result<()> {
        info!("Resetting knowledge database...");

        // åˆ é™¤æ‰€æœ‰å‘é‡
        sqlx::query("DELETE FROM knowledge_vectors").execute(self.knowledge_pool()).await?;

        // åˆ é™¤æ‰€æœ‰åˆ†å—
        sqlx::query("DELETE FROM knowledge_chunks").execute(self.knowledge_pool()).await?;

        // åˆ é™¤æ‰€æœ‰æ–‡æ¡£
        sqlx::query("DELETE FROM knowledge_documents").execute(self.knowledge_pool()).await?;

        // åˆ é™¤æ‰€æœ‰é›†åˆ
        sqlx::query("DELETE FROM knowledge_collections").execute(self.knowledge_pool()).await?;

        // æ¸…ç†ç¼“å­˜
        self.clear_cache();

        info!("Knowledge database reset completed");
        Ok(())
    }

    // å®Œå…¨é‡ç½®æ•°æ®åº“ï¼ˆåŒ…æ‹¬ä¸»æ•°æ®åº“ï¼‰
    pub async fn reset_all_databases(&self) -> Result<()> {
        info!("Resetting all databases...");

        // é‡ç½®çŸ¥è¯†åº“æ•°æ®åº“
        self.reset_knowledge_database().await?;

        // åˆ é™¤æ‰€æœ‰è§’è‰²
        sqlx::query("DELETE FROM roles").execute(self.main_pool()).await?;

        // åˆ é™¤æ‰€æœ‰æ¨¡å‹ç»„
        sqlx::query("DELETE FROM model_groups").execute(self.main_pool()).await?;

        // é‡æ–°æ’å…¥é»˜è®¤çš„åµŒå…¥æ¨¡å‹
        let default_models = vec![
            ("bge-m3", "BAAI/bge-m3", "BAAI/bge-m3", 1024, "default", 8192),
            ("bge-large-zh", "BAAI/bge-large-zh-v1.5", "BAAI/bge-large-zh-v1.5", 1024, "zh", 8192),
            ("bge-large-en", "BAAI/bge-large-en-v1.5", "BAAI/bge-large-en-v1.5", 1024, "en", 8192),
        ];

        let now = chrono::Utc::now().timestamp();
        for (id, name, model_id, dimensions, language, max_tokens) in default_models {
            sqlx::query(
                "INSERT OR REPLACE INTO embedding_models (id, name, model_id, dimensions, language, enabled, max_tokens, created_at, updated_at)
                 VALUES (?, ?, ?, ?, ?, 1, ?, ?, ?)"
            )
            .bind(id)
            .bind(name)
            .bind(model_id)
            .bind(dimensions)
            .bind(language)
            .bind(max_tokens)
            .bind(now)
            .bind(now)
            .execute(self.main_pool())
            .await?;
        }

        info!("All databases reset completed");
        Ok(())
    }

    // æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜
    pub fn clear_query_cache(&self) {
        let mut cache = self.query_cache.lock().unwrap();
        let cache_size = cache.len();
        cache.clear();
        println!("ğŸ—‘ï¸ [ç¼“å­˜] æŸ¥è¯¢ç¼“å­˜å·²æ¸…ç©ºï¼Œæ¸…ç©ºäº† {} ä¸ªæ¡ç›®", cache_size);
    }
}

// æ•°æ®åº“å¥åº·æ£€æŸ¥
impl DatabaseManager {
    pub async fn health_check(&self) -> Result<DatabaseHealth> {
        let mut health = DatabaseHealth {
            main_db: false,
            knowledge_db: false,
            vec_extension: false,
            cache_stats: (0, 0),
        };

        // æ£€æŸ¥ä¸»æ•°æ®åº“
        match sqlx::query("SELECT 1").fetch_one(self.main_pool()).await {
            Ok(_) => health.main_db = true,
            Err(e) => warn!("Main database health check failed: {}", e),
        }

        // æ£€æŸ¥çŸ¥è¯†åº“æ•°æ®åº“
        match sqlx::query("SELECT 1").fetch_one(self.knowledge_pool()).await {
            Ok(_) => health.knowledge_db = true,
            Err(e) => warn!("Knowledge database health check failed: {}", e),
        }

        // æ£€æŸ¥sqlite-vecæ‰©å±•
        match sqlx::query("SELECT vec_version()").fetch_one(self.knowledge_pool()).await {
            Ok(_) => health.vec_extension = true,
            Err(e) => {
                warn!("sqlite-vec extension health check failed: {}", e);
                health.vec_extension = false;
            }
        }

        // è·å–ç¼“å­˜ç»Ÿè®¡
        health.cache_stats = self.cache_stats();

        Ok(health)
    }

    // è·å–å¯¹è¯æ•°é‡
    pub async fn get_conversation_count(&self) -> Result<i64> {
        let result = sqlx::query("SELECT COUNT(*) as count FROM conversations")
            .fetch_one(self.main_pool())
            .await
            .map_err(|e| anyhow!("è·å–å¯¹è¯æ•°é‡å¤±è´¥: {}", e))?;

        Ok(result.get("count"))
    }

    // è·å–æ¶ˆæ¯æ•°é‡
    pub async fn get_message_count(&self) -> Result<i64> {
        let result = sqlx::query("SELECT COUNT(*) as count FROM messages")
            .fetch_one(self.main_pool())
            .await
            .map_err(|e| anyhow!("è·å–æ¶ˆæ¯æ•°é‡å¤±è´¥: {}", e))?;

        Ok(result.get("count"))
    }

    // è·å–APIä¼šè¯æ•°é‡
    pub async fn get_api_session_count(&self) -> Result<i64> {
        // ç»Ÿè®¡è®¾ç½®è¡¨ä¸­çš„APIç›¸å…³è®¾ç½®é¡¹æ•°é‡ä½œä¸ºæ›¿ä»£
        let result = sqlx::query("SELECT COUNT(*) as count FROM settings WHERE key LIKE 'api_%'")
            .fetch_one(self.main_pool())
            .await
            .map_err(|e| anyhow!("è·å–APIä¼šè¯æ•°é‡å¤±è´¥: {}", e))?;

        Ok(result.get("count"))
    }

    // è·å–çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡
    pub async fn get_document_count(&self) -> Result<i64> {
        let result = sqlx::query("SELECT COUNT(*) as count FROM documents")
            .fetch_one(self.knowledge_pool())
            .await
            .map_err(|e| anyhow!("è·å–æ–‡æ¡£æ•°é‡å¤±è´¥: {}", e))?;

        Ok(result.get("count"))
    }

    // è·å–è®¾ç½®æ•°é‡
    pub async fn get_setting_count(&self) -> Result<i64> {
        let result = sqlx::query("SELECT COUNT(*) as count FROM settings")
            .fetch_one(self.main_pool())
            .await
            .map_err(|e| anyhow!("è·å–è®¾ç½®æ•°é‡å¤±è´¥: {}", e))?;

        Ok(result.get("count"))
    }

    // è·å–æ•°æ®åº“æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub async fn get_database_size(&self) -> Result<i64> {
        // è·å–ä¸»æ•°æ®åº“æ–‡ä»¶å¤§å°
        let main_size = sqlx::query("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            .fetch_one(self.main_pool())
            .await
            .map(|row| row.get::<i64, _>("size"))
            .unwrap_or(0);

        // è·å–çŸ¥è¯†åº“æ•°æ®åº“æ–‡ä»¶å¤§å°
        let knowledge_size = sqlx::query("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            .fetch_one(self.knowledge_pool())
            .await
            .map(|row| row.get::<i64, _>("size"))
            .unwrap_or(0);

        Ok(main_size + knowledge_size)
    }
}

// å¯¹è¯ç®¡ç†æ–¹æ³•
impl DatabaseManager {
    // ä¿å­˜å¯¹è¯
    pub async fn save_conversation(&self, conversation: &Conversation) -> Result<()> {
        let query = sqlx::query(
            "INSERT OR REPLACE INTO conversations (id, title, role_id, response_mode, messages, settings, is_favorite, pinned_at, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        )
        .bind(&conversation.id)
        .bind(&conversation.title)
        .bind(&conversation.role_id)
        .bind(&conversation.response_mode)
        .bind(&conversation.messages)
        .bind(&conversation.settings)
        .bind(&conversation.is_favorite)
        .bind(&conversation.pinned_at)
        .bind(&conversation.created_at)
        .bind(&conversation.updated_at);

        query.execute(self.main_pool()).await?;
        Ok(())
    }

    // è·å–æ‰€æœ‰å¯¹è¯
    pub async fn get_conversations(&self) -> Result<Vec<Conversation>> {
        let rows = sqlx::query_as::<_, Conversation>(
            "SELECT * FROM conversations ORDER BY is_favorite DESC, pinned_at DESC, created_at DESC"
        )
        .fetch_all(self.main_pool())
        .await?;

        Ok(rows)
    }

    // åˆ é™¤å¯¹è¯
    pub async fn delete_conversation(&self, conversation_id: &str) -> Result<()> {
        sqlx::query("DELETE FROM conversations WHERE id = ?")
            .bind(conversation_id)
            .execute(self.main_pool())
            .await?;
        Ok(())
    }

    // æ¸…ç©ºæ‰€æœ‰å¯¹è¯
    pub async fn clear_conversations(&self) -> Result<()> {
        sqlx::query("DELETE FROM conversations").execute(self.main_pool()).await?;
        Ok(())
    }

    // åˆ‡æ¢å¯¹è¯æ”¶è—çŠ¶æ€
    pub async fn toggle_conversation_favorite(&self, conversation_id: &str) -> Result<bool> {
        // å…ˆè·å–å½“å‰çŠ¶æ€
        let current = sqlx::query_as::<_, Conversation>(
            "SELECT * FROM conversations WHERE id = ?"
        )
        .bind(conversation_id)
        .fetch_optional(self.main_pool())
        .await?;

        if let Some(conversation) = current {
            let new_favorite = !conversation.is_favorite;
            let pinned_at = if new_favorite {
                Some(chrono::Utc::now().to_rfc3339())
            } else {
                None
            };

            sqlx::query(
                "UPDATE conversations SET is_favorite = ?, pinned_at = ?, updated_at = ? WHERE id = ?"
            )
            .bind(new_favorite)
            .bind(&pinned_at)
            .bind(&chrono::Utc::now().to_rfc3339())
            .bind(conversation_id)
            .execute(self.main_pool())
            .await?;

            Ok(new_favorite)
        } else {
            Err(anyhow!("Conversation not found"))
        }
    }

    // è·å–æ”¶è—çš„å¯¹è¯
    pub async fn get_favorite_conversations(&self) -> Result<Vec<Conversation>> {
        let rows = sqlx::query_as::<_, Conversation>(
            "SELECT * FROM conversations WHERE is_favorite = TRUE ORDER BY pinned_at DESC, created_at DESC"
        )
        .fetch_all(self.main_pool())
        .await?;

        Ok(rows)
    }
}

// è®¾ç½®ç®¡ç†æ–¹æ³•
impl DatabaseManager {
    // ä¿å­˜è®¾ç½®
    pub async fn save_setting(&self, key: &str, value: &str) -> Result<()> {
        let now = chrono::Utc::now().to_rfc3339();
        let query = sqlx::query(
            "INSERT OR REPLACE INTO settings (key, value, updated_at) VALUES (?, ?, ?)"
        )
        .bind(key)
        .bind(value)
        .bind(&now);

        query.execute(self.main_pool()).await?;
        Ok(())
    }

    // è·å–è®¾ç½®
    pub async fn get_setting(&self, key: &str) -> Result<Option<String>> {
        let row = sqlx::query("SELECT value FROM settings WHERE key = ?")
            .bind(key)
            .fetch_optional(self.main_pool())
            .await?;

        Ok(row.map(|r| r.get("value")))
    }

    // è·å–æ‰€æœ‰è®¾ç½®
    pub async fn get_all_settings(&self) -> Result<Vec<(String, String)>> {
        let rows = sqlx::query("SELECT key, value FROM settings")
            .fetch_all(self.main_pool())
            .await?;

        let settings = rows.into_iter()
            .map(|row| (row.get("key"), row.get("value")))
            .collect();

        Ok(settings)
    }
}

// è§’è‰²ç®¡ç†æ–¹æ³•
impl DatabaseManager {
    // ä¿å­˜è§’è‰²
    pub async fn save_role(&self, role: &Role) -> Result<()> {
        let query = sqlx::query(
            "INSERT OR REPLACE INTO roles (id, name, icon, avatar, description, temperature, system_prompt, color, sort_order, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        )
        .bind(&role.id)
        .bind(&role.name)
        .bind(&role.icon)
        .bind(&role.avatar)
        .bind(&role.description)
        .bind(role.temperature)
        .bind(&role.system_prompt)
        .bind(&role.color)
        .bind(role.sort_order)
        .bind(&role.created_at)
        .bind(&role.updated_at);

        query.execute(self.main_pool()).await?;
        Ok(())
    }

    // è·å–æ‰€æœ‰è§’è‰²
    pub async fn get_roles(&self) -> Result<Vec<Role>> {
        let rows = sqlx::query_as::<_, Role>(
            "SELECT * FROM roles ORDER BY sort_order ASC, created_at ASC"
        )
        .fetch_all(self.main_pool())
        .await?;

        Ok(rows)
    }

    // åˆ é™¤è§’è‰²
    pub async fn delete_role(&self, role_id: &str) -> Result<()> {
        sqlx::query("DELETE FROM roles WHERE id = ?")
            .bind(role_id)
            .execute(self.main_pool())
            .await?;
        Ok(())
    }
}

// æ¨¡å‹ç®¡ç†æ–¹æ³•
impl DatabaseManager {
    // ä¿å­˜æ¨¡å‹åˆ†ç»„
    pub async fn save_model_group(&self, group: &ModelGroup) -> Result<()> {
        let query = sqlx::query(
            "INSERT OR REPLACE INTO model_groups (id, name, provider, description, sort_order, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?, ?, ?)"
        )
        .bind(&group.id)
        .bind(&group.name)
        .bind(&group.provider)
        .bind(&group.description)
        .bind(group.sort_order)
        .bind(&group.created_at)
        .bind(&group.updated_at);

        query.execute(self.main_pool()).await?;
        Ok(())
    }

    // è·å–æ‰€æœ‰æ¨¡å‹åˆ†ç»„
    pub async fn get_model_groups(&self) -> Result<Vec<ModelGroup>> {
        let rows = sqlx::query_as::<_, ModelGroup>(
            "SELECT * FROM model_groups ORDER BY sort_order ASC, created_at ASC"
        )
        .fetch_all(self.main_pool())
        .await?;

        Ok(rows)
    }

    // åˆ é™¤æ¨¡å‹åˆ†ç»„
    pub async fn delete_model_group(&self, group_id: &str) -> Result<()> {
        sqlx::query("DELETE FROM model_groups WHERE id = ?")
            .bind(group_id)
            .execute(self.main_pool())
            .await?;
        Ok(())
    }

    // ä¿å­˜æ¨¡å‹
    pub async fn save_model(&self, model: &Model) -> Result<()> {
        let query = sqlx::query(
            "INSERT OR REPLACE INTO models (id, group_id, name, model_id, enabled, description, api_params, sort_order, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        )
        .bind(&model.id)
        .bind(&model.group_id)
        .bind(&model.name)
        .bind(&model.model_id)
        .bind(model.enabled)
        .bind(&model.description)
        .bind(&model.api_params)
        .bind(model.sort_order)
        .bind(&model.created_at)
        .bind(&model.updated_at);

        query.execute(self.main_pool()).await?;
        Ok(())
    }

    // è·å–æ‰€æœ‰æ¨¡å‹
    pub async fn get_models(&self) -> Result<Vec<Model>> {
        let rows = sqlx::query_as::<_, Model>(
            "SELECT * FROM models ORDER BY sort_order ASC, created_at ASC"
        )
        .fetch_all(self.main_pool())
        .await?;

        Ok(rows)
    }

    // åˆ é™¤æ¨¡å‹
    pub async fn delete_model(&self, model_id: &str) -> Result<()> {
        sqlx::query("DELETE FROM models WHERE id = ?")
            .bind(model_id)
            .execute(self.main_pool())
            .await?;
        Ok(())
    }
}

mod tests {
    use super::*;

    #[tokio::test]
    async fn test_database_initialization() {
        let db = DatabaseManager::new().await.unwrap();
        assert!(db.health_check().await.unwrap().main_db);
    }
}