// ç§»é™¤databaseæ¨¡å—ï¼Œä½¿ç”¨tauri-plugin-sql

use serde::{Deserialize, Serialize};
use anyhow::Result;
use sqlx::{SqlitePool, Row};
use std::sync::Mutex;
use tauri::Manager;

mod qdrant_manager;
mod qdrant_service;
mod document_processor;
mod siliconflow_embedding;
use document_processor::{DocumentProcessor, process_document_embeddings, get_processing_progress, chunk_document_text};
use siliconflow_embedding::{
    generate_siliconflow_embedding, generate_siliconflow_batch_embeddings
};
// ä½¿ç”¨å‰ç«¯çš„francè¯­è¨€æ£€æµ‹ï¼Œä¸å†éœ€è¦Rustç«¯çš„è¯­è¨€æ£€æµ‹

// ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹ç›¸å…³å‘½ä»¤

// ç”Ÿæˆå•ä¸ªåµŒå…¥å‘é‡
#[tauri::command]
async fn generate_siliconflow_embedding_cmd(api_key: String, text: String, model: Option<String>) -> Result<Vec<f32>, String> {
  let model_name = model.unwrap_or_else(|| "BAAI/bge-m3".to_string());
  generate_siliconflow_embedding(api_key, text, model_name).await
}

// æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
#[tauri::command]
async fn generate_siliconflow_batch_embeddings_cmd(api_key: String, texts: Vec<String>, model: Option<String>) -> Result<Vec<Vec<f32>>, String> {
  let model_name = model.unwrap_or_else(|| "BAAI/bge-m3".to_string());
  generate_siliconflow_batch_embeddings(api_key, texts, model_name).await
}

// è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
#[tauri::command]
async fn get_siliconflow_models_cmd() -> Result<Vec<String>, String> {
  Ok(vec![
    "BAAI/bge-large-zh-v1.5".to_string(),
    "BAAI/bge-large-en-v1.5".to_string(),
    "BAAI/bge-m3".to_string(),
  ])
}

// è¯­è¨€æ£€æµ‹åŠŸèƒ½ç§»è‡³å‰ç«¯ï¼Œä½¿ç”¨francåº“

// å–æ¶ˆå½“å‰åµŒå…¥ä»»åŠ¡ - å·²ç§»é™¤æœ¬åœ°æ¨¡å‹ï¼Œæ­¤å‡½æ•°ä¿ç•™ä¸ºå…¼å®¹æ€§
#[tauri::command]
fn cancel_embedding_jobs(_state: tauri::State<'_, std::sync::Mutex<()>>) {
    println!("ğŸ›‘ æœ¬åœ°åµŒå…¥æ¨¡å‹å·²ç§»é™¤ï¼Œå–æ¶ˆæŒ‡ä»¤æ— æ•ˆ");
}
use qdrant_manager::{
    QdrantManager,
    compile_qdrant,
    start_qdrant,
    stop_qdrant,
    get_qdrant_status,
    is_qdrant_installed,
    get_qdrant_version
};
use qdrant_service::QdrantService;

#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
  tauri::Builder::default()
    .plugin(tauri_plugin_fs::init())
    .plugin(tauri_plugin_shell::init())
    .plugin(tauri_plugin_sql::Builder::default().build())
    .manage(Mutex::new(QdrantManager::new()))
    .manage(Mutex::new(QdrantService::new()))
    .manage(Mutex::new(DocumentProcessor::new()))
    .setup(|app| {
      if cfg!(debug_assertions) {
        app.handle().plugin(
          tauri_plugin_log::Builder::default()
            .level(log::LevelFilter::Info)
            .build(),
        )?;
      }
      
      // è‡ªåŠ¨å¯åŠ¨Qdrant (ä½¿ç”¨é¢„ç¼–è¯‘äºŒè¿›åˆ¶æ–‡ä»¶)
      let app_handle = app.handle().clone();
      std::thread::spawn(move || {
        let manager = app_handle.state::<Mutex<QdrantManager>>();
        let mut manager = manager.lock().unwrap();
        
        // æ£€æŸ¥é¢„ç¼–è¯‘çš„äºŒè¿›åˆ¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        let current_dir = std::env::current_dir().unwrap();
        println!("ğŸ” å½“å‰å·¥ä½œç›®å½•: {}", current_dir.display());
        println!("ğŸ” Qdrantæ–‡ä»¶æ˜¯å¦å­˜åœ¨: {}", manager.is_installed());
        
        if manager.is_installed() {
          println!("ğŸš€ å¯åŠ¨é¢„ç¼–è¯‘çš„QdrantæœåŠ¡...");
          match manager.start() {
            Ok(_) => println!("âœ… QdrantæœåŠ¡å·²è‡ªåŠ¨å¯åŠ¨"),
            Err(e) => println!("âŒ QdrantæœåŠ¡å¯åŠ¨å¤±è´¥: {}", e),
          }
        } else {
          println!("âš ï¸ æœªæ‰¾åˆ°é¢„ç¼–è¯‘çš„QdrantäºŒè¿›åˆ¶æ–‡ä»¶");
          println!("ğŸ’¡ è¯·å…ˆè¿è¡Œ: .\\compile_qdrant.bat æ¥ç¼–è¯‘Qdrant");
          println!("ğŸ’¡ æˆ–è€…æ‰‹åŠ¨å¯åŠ¨: .\\qdrant.exe");
        }
      });
      
      // MiniLMæ¨¡å‹å·²è¢«å¼ƒç”¨ï¼Œä¸å†æ£€æµ‹
      
      // ç¡…åŸºæµåŠ¨APIæ— éœ€åˆå§‹åŒ–ï¼Œç›´æ¥HTTPè°ƒç”¨
      println!("âœ… ç¡…åŸºæµåŠ¨APIå‡†å¤‡å°±ç»ªï¼Œæ— éœ€åˆå§‹åŒ–");
      
      Ok(())
    })
    .invoke_handler(tauri::generate_handler![
      ensure_data_directory,
      get_file_size,
      init_knowledge_base,
      add_knowledge_document,
      add_knowledge_vector,
      search_knowledge_base,
      search_knowledge_base_with_documents,
      get_knowledge_documents,
      delete_knowledge_document,
      get_knowledge_statistics,
      generate_document_embeddings,
      process_document_embeddings,
      get_processing_progress,
      chunk_document_text,
      generate_siliconflow_embedding_cmd,
      generate_siliconflow_batch_embeddings_cmd,
      get_siliconflow_models_cmd,
      cancel_embedding_jobs,
      compile_qdrant,
      start_qdrant,
      stop_qdrant,
      get_qdrant_status,
      is_qdrant_installed,
      get_qdrant_version,
    ])
    .run(tauri::generate_context!())
    .expect("error while running tauri application");
}

#[derive(Debug, Serialize, Deserialize)]
struct KnowledgeDocument {
    id: String,
    title: String,
    content: String,
    source_type: String,
    source_url: Option<String>,
    file_path: Option<String>,
    file_size: Option<i64>,
    mime_type: Option<String>,
    metadata: Option<String>,
    created_at: i64,
    updated_at: i64,
}

#[derive(Debug, Serialize, Deserialize)]
struct KnowledgeVector {
    vector_id: String,
    document_id: String,
    chunk_index: i32,
    chunk_text: String,
    embedding: Vec<f32>,
    created_at: i64,
}

#[derive(Debug, Serialize, Deserialize)]
struct SearchResult {
    document_id: String,
    title: String,
    content: String,
    chunk_text: String,
    score: f32,
    metadata: Option<String>,
}

#[tauri::command]
async fn ensure_data_directory() -> Result<String, String> {
  use std::fs;
  
  let data_dir = if cfg!(debug_assertions) {
    "./data".to_string()
  } else {
    let app_data = std::env::var("APPDATA").unwrap_or_else(|_| ".".to_string());
    format!("{}/ai_chat", app_data)
  };
  
  fs::create_dir_all(&data_dir).map_err(|e| format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e))?;
  Ok(data_dir)
}

#[tauri::command]
async fn get_file_size(file_path: String) -> Result<u64, String> {
    use std::fs;
    let metadata = fs::metadata(&file_path)
        .map_err(|e| format!("æ— æ³•è·å–æ–‡ä»¶å…ƒæ•°æ®: {}", e))?;
    Ok(metadata.len())
}

#[tauri::command]
async fn init_knowledge_base() -> Result<String, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;

    // åˆ›å»ºçŸ¥è¯†åº“æ–‡æ¡£è¡¨
    sqlx::query(
        r#"
        CREATE TABLE IF NOT EXISTS knowledge_documents (
            id TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            content TEXT NOT NULL,
            source_type TEXT NOT NULL,
            source_url TEXT,
            file_path TEXT,
            file_size INTEGER,
            mime_type TEXT,
            metadata TEXT,
            created_at INTEGER NOT NULL,
            updated_at INTEGER NOT NULL
        )
        "#
    )
    .execute(&pool)
    .await
    .map_err(|e| format!("åˆ›å»ºçŸ¥è¯†åº“æ–‡æ¡£è¡¨å¤±è´¥: {}", e))?;
    
    // åˆ›å»ºsqlite-vecè™šæ‹Ÿè¡¨ç”¨äºå‘é‡æœç´¢
    sqlx::query(
        r#"
        CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_vectors USING vec0(
            embedding float[768]
        )
        "#
    )
    .execute(&pool)
    .await
    .map_err(|e| format!("åˆ›å»ºå‘é‡è™šæ‹Ÿè¡¨å¤±è´¥: {}", e))?;
    
    // åˆ›å»ºå‘é‡å…ƒæ•°æ®è¡¨
    sqlx::query(
        r#"
        CREATE TABLE IF NOT EXISTS vector_metadata (
            rowid INTEGER PRIMARY KEY,
            document_id TEXT NOT NULL,
            chunk_index INTEGER NOT NULL,
            chunk_text TEXT NOT NULL,
            created_at INTEGER NOT NULL,
            FOREIGN KEY (document_id) REFERENCES knowledge_documents(id) ON DELETE CASCADE
        )
        "#
    )
    .execute(&pool)
    .await
    .map_err(|e| format!("åˆ›å»ºå‘é‡å…ƒæ•°æ®è¡¨å¤±è´¥: {}", e))?;
    
    // åˆ›å»ºç´¢å¼•ä»¥æé«˜æœç´¢æ€§èƒ½
    sqlx::query("CREATE INDEX IF NOT EXISTS idx_knowledge_documents_title ON knowledge_documents(title)")
        .execute(&pool)
        .await
        .map_err(|e| format!("åˆ›å»ºæ ‡é¢˜ç´¢å¼•å¤±è´¥: {}", e))?;
    
    sqlx::query("CREATE INDEX IF NOT EXISTS idx_knowledge_documents_content ON knowledge_documents(content)")
        .execute(&pool)
        .await
        .map_err(|e| format!("åˆ›å»ºå†…å®¹ç´¢å¼•å¤±è´¥: {}", e))?;
    
    pool.close().await;
    
    Ok("çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸ".to_string())
}

#[tauri::command]
async fn add_knowledge_document(document: KnowledgeDocument) -> Result<String, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    sqlx::query(
        r#"
        INSERT OR REPLACE INTO knowledge_documents 
        (id, title, content, source_type, source_url, file_path, file_size, mime_type, metadata, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        "#
    )
    .bind(&document.id)
    .bind(&document.title)
    .bind(&document.content)
    .bind(&document.source_type)
    .bind(&document.source_url)
    .bind(&document.file_path)
    .bind(&document.file_size)
    .bind(&document.mime_type)
    .bind(&document.metadata)
    .bind(document.created_at)
    .bind(document.updated_at)
    .execute(&pool)
    .await
    .map_err(|e| format!("æ·»åŠ çŸ¥è¯†åº“æ–‡æ¡£å¤±è´¥: {}", e))?;
    
    pool.close().await;
    Ok(document.id)
}

#[tauri::command]
async fn add_knowledge_vector(vector: KnowledgeVector) -> Result<String, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    // å°†å‘é‡è½¬æ¢ä¸ºJSONæ ¼å¼ï¼ˆsqlite-vecè¦æ±‚çš„æ ¼å¼ï¼‰
    let embedding_json = serde_json::to_string(&vector.embedding)
        .map_err(|e| format!("åºåˆ—åŒ–å‘é‡å¤±è´¥: {}", e))?;
    
    // æ’å…¥å‘é‡åˆ°è™šæ‹Ÿè¡¨
    let result = sqlx::query(
        "INSERT INTO knowledge_vectors (embedding) VALUES (?)"
    )
    .bind(&embedding_json)
    .execute(&pool)
    .await
    .map_err(|e| format!("æ·»åŠ å‘é‡å¤±è´¥: {}", e))?;
    
    let rowid = result.last_insert_rowid();
    
    // æ’å…¥å…ƒæ•°æ®åˆ°å…ƒæ•°æ®è¡¨
    sqlx::query(
        "INSERT INTO vector_metadata (rowid, document_id, chunk_index, chunk_text, created_at) VALUES (?, ?, ?, ?, ?)"
    )
    .bind(rowid)
    .bind(&vector.document_id)
    .bind(vector.chunk_index)
    .bind(&vector.chunk_text)
    .bind(vector.created_at)
    .execute(&pool)
    .await
    .map_err(|e| format!("æ·»åŠ å‘é‡å…ƒæ•°æ®å¤±è´¥: {}", e))?;
    
    pool.close().await;
    Ok(vector.vector_id)
}

#[tauri::command]
async fn search_knowledge_base(query: String, limit: Option<i32>) -> Result<Vec<SearchResult>, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    // ç”ŸæˆæŸ¥è¯¢å‘é‡
    let query_embedding = generate_simple_embedding(&query).await;
    let query_embedding_json = serde_json::to_string(&query_embedding)
        .map_err(|e| format!("åºåˆ—åŒ–æŸ¥è¯¢å‘é‡å¤±è´¥: {}", e))?;
    
    let limit = limit.unwrap_or(10);
    
    // ä½¿ç”¨sqlite-vecè¿›è¡Œå‘é‡æœç´¢
    let results = sqlx::query(
        r#"
        SELECT 
            vm.document_id,
            kd.title,
            kd.content,
            vm.chunk_text,
            distance,
            kd.metadata
        FROM knowledge_vectors kv
        JOIN vector_metadata vm ON kv.rowid = vm.rowid
        JOIN knowledge_documents kd ON vm.document_id = kd.id
        WHERE kv.embedding MATCH ?
        ORDER BY distance ASC
        LIMIT ?
        "#
    )
    .bind(&query_embedding_json)
    .bind(limit)
    .fetch_all(&pool)
    .await
    .map_err(|e| format!("å‘é‡æœç´¢å¤±è´¥: {}", e))?;
    
    let search_results: Vec<SearchResult> = results
        .iter()
        .map(|row| SearchResult {
            document_id: row.get("document_id"),
            title: row.get("title"),
            content: row.get("content"),
            chunk_text: row.get("chunk_text"),
            score: 1.0 - row.get::<f32, _>("distance"), // è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
            metadata: row.get("metadata"),
        })
        .collect();
    
    pool.close().await;
    Ok(search_results)
}

#[tauri::command]
async fn search_knowledge_base_with_documents(
    query: String, 
    document_ids: Option<Vec<String>>, 
    limit: Option<i32>
) -> Result<Vec<SearchResult>, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    // ç”ŸæˆæŸ¥è¯¢å‘é‡
    let query_embedding = generate_simple_embedding(&query).await;
    let query_embedding_json = serde_json::to_string(&query_embedding)
        .map_err(|e| format!("åºåˆ—åŒ–æŸ¥è¯¢å‘é‡å¤±è´¥: {}", e))?;
    
    let limit = limit.unwrap_or(10);
    
    // æ„å»ºæŸ¥è¯¢SQLï¼Œæ”¯æŒæ–‡æ¡£è¿‡æ»¤
    let mut query_sql = String::from(
        r#"
        SELECT 
            vm.document_id,
            kd.title,
            kd.content,
            vm.chunk_text,
            distance,
            kd.metadata
        FROM knowledge_vectors kv
        JOIN vector_metadata vm ON kv.rowid = vm.rowid
        JOIN knowledge_documents kd ON vm.document_id = kd.id
        WHERE kv.embedding MATCH ?
        "#
    );
    
    // å¦‚æœæŒ‡å®šäº†æ–‡æ¡£IDï¼Œæ·»åŠ è¿‡æ»¤æ¡ä»¶
    if let Some(ref doc_ids) = document_ids {
        if !doc_ids.is_empty() {
            let placeholders: Vec<String> = doc_ids.iter().map(|_| "?".to_string()).collect();
            let id_list: Vec<&String> = doc_ids.iter().collect();
            query_sql.push_str(&format!(" AND vm.document_id IN ({})", placeholders.join(", ")));
            
            let mut query = sqlx::query(&query_sql);
            query = query.bind(query_embedding_json);
            for id in id_list {
                query = query.bind(id);
            }
            query = query.bind(limit);
            let results = query.fetch_all(&pool)
                .await
                .map_err(|e| format!("æœç´¢çŸ¥è¯†åº“å¤±è´¥: {}", e))?;
            
            let search_results: Vec<SearchResult> = results
                .iter()
                .map(|row| SearchResult {
                    document_id: row.get("document_id"),
                    title: row.get("title"),
                    content: row.get("content"),
                    chunk_text: row.get("chunk_text"),
                    score: 1.0 - row.get::<f32, _>("distance"), // è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    metadata: row.get("metadata"),
                })
                .collect();
            
            pool.close().await;
            Ok(search_results)
        } else {
            // å¦‚æœæ²¡æœ‰é€‰æ‹©æ–‡æ¡£ï¼Œæ‰§è¡Œæ™®é€šæœç´¢
            let results = sqlx::query(
                &format!(r#"
                SELECT 
                    vm.document_id,
                    kd.title,
                    kd.content,
                    vm.chunk_text,
                    distance,
                    kd.metadata
                FROM knowledge_vectors kv
                JOIN vector_metadata vm ON kv.rowid = vm.rowid
                JOIN knowledge_documents kd ON vm.document_id = kd.id
                WHERE kv.embedding MATCH ?
                ORDER BY distance ASC
                LIMIT ?
                "#)
            )
            .bind(query_embedding_json)
            .bind(limit)
            .fetch_all(&pool)
            .await
            .map_err(|e| format!("æœç´¢çŸ¥è¯†åº“å¤±è´¥: {}", e))?;
            
            let search_results: Vec<SearchResult> = results
                .iter()
                .map(|row| SearchResult {
                    document_id: row.get("document_id"),
                    title: row.get("title"),
                    content: row.get("content"),
                    chunk_text: row.get("chunk_text"),
                    score: 1.0 - row.get::<f32, _>("distance"), // è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    metadata: row.get("metadata"),
                })
                .collect();
            
            pool.close().await;
            Ok(search_results)
        }
    } else {
        // å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡æ¡£IDï¼Œæ‰§è¡Œæ™®é€šæœç´¢
        let results = sqlx::query(
            &format!(r#"
            SELECT 
                vm.document_id,
                kd.title,
                kd.content,
                vm.chunk_text,
                distance,
                kd.metadata
            FROM knowledge_vectors kv
            JOIN vector_metadata vm ON kv.rowid = vm.rowid
            JOIN knowledge_documents kd ON vm.document_id = kd.id
            WHERE kv.embedding MATCH ?
            ORDER BY distance ASC
            LIMIT ?
            "#)
        )
        .bind(query_embedding_json)
        .bind(limit)
        .fetch_all(&pool)
        .await
        .map_err(|e| format!("æœç´¢çŸ¥è¯†åº“å¤±è´¥: {}", e))?;
        
        let search_results: Vec<SearchResult> = results
            .iter()
            .map(|row| SearchResult {
                document_id: row.get("document_id"),
                title: row.get("title"),
                content: row.get("content"),
                chunk_text: row.get("chunk_text"),
                score: 1.0 - row.get::<f32, _>("distance"), // è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                metadata: row.get("metadata"),
            })
            .collect();
        
        pool.close().await;
        Ok(search_results)
    }
}

// ä½¿ç”¨ç¡…åŸºæµåŠ¨APIç”ŸæˆåµŒå…¥å‘é‡
async fn generate_simple_embedding(text: &str) -> Vec<f32> {
    // ä½¿ç”¨é»˜è®¤çš„å¤šè¯­è¨€æ¨¡å‹
    let model_name = "BAAI/bge-m3";
    match generate_siliconflow_embedding("dummy_key".to_string(), text.to_string(), model_name.to_string()).await {
        Ok(embedding) => embedding,
        Err(e) => {
            println!("âŒ ç¡…åŸºæµåŠ¨åµŒå…¥ç”Ÿæˆå¤±è´¥: {}, ä½¿ç”¨å¤‡ç”¨æ–¹æ³•", e);
            // å¤‡ç”¨æ–¹æ³•ï¼šç”Ÿæˆç®€å•çš„éšæœºå‘é‡
            let mut embedding = vec![0.0; 768];
            let mut hash = 0u64;
            
            for (_i, byte) in text.bytes().enumerate() {
                hash = hash.wrapping_mul(31).wrapping_add(byte as u64);
                let index = (hash as usize) % 768;
                embedding[index] = (hash % 1000) as f32 / 1000.0 - 0.5;
            }
            
            // å½’ä¸€åŒ–å‘é‡
            let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
            if norm > 0.0 {
                for val in &mut embedding {
                    *val /= norm;
                }
            }
            
            embedding
        }
    }
}

#[tauri::command]
async fn get_knowledge_documents() -> Result<Vec<KnowledgeDocument>, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    let results = sqlx::query(
        r#"
        SELECT * FROM knowledge_documents 
        ORDER BY updated_at DESC
        "#
    )
    .fetch_all(&pool)
    .await
    .map_err(|e| format!("è·å–çŸ¥è¯†åº“æ–‡æ¡£å¤±è´¥: {}", e))?;
    
    let documents: Vec<KnowledgeDocument> = results
        .iter()
        .map(|row| KnowledgeDocument {
            id: row.get("id"),
            title: row.get("title"),
            content: row.get("content"),
            source_type: row.get("source_type"),
            source_url: row.get("source_url"),
            file_path: row.get("file_path"),
            file_size: row.get("file_size"),
            mime_type: row.get("mime_type"),
            metadata: row.get("metadata"),
            created_at: row.get("created_at"),
            updated_at: row.get("updated_at"),
        })
        .collect();
    
    pool.close().await;
    Ok(documents)
}

#[tauri::command]
async fn delete_knowledge_document(document_id: String) -> Result<String, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    // è·å–è¦åˆ é™¤çš„å‘é‡rowid
    let vector_rowids: Vec<i64> = sqlx::query_scalar(
        "SELECT rowid FROM vector_metadata WHERE document_id = ?"
    )
    .bind(&document_id)
    .fetch_all(&pool)
    .await
    .map_err(|e| format!("è·å–å‘é‡rowidå¤±è´¥: {}", e))?;
    
    // åˆ é™¤å‘é‡æ•°æ®
    for rowid in vector_rowids {
        sqlx::query("DELETE FROM knowledge_vectors WHERE rowid = ?")
            .bind(rowid)
            .execute(&pool)
            .await
            .map_err(|e| format!("åˆ é™¤å‘é‡å¤±è´¥: {}", e))?;
    }
    
    // åˆ é™¤å‘é‡å…ƒæ•°æ®
    sqlx::query("DELETE FROM vector_metadata WHERE document_id = ?")
        .bind(&document_id)
        .execute(&pool)
        .await
        .map_err(|e| format!("åˆ é™¤å‘é‡å…ƒæ•°æ®å¤±è´¥: {}", e))?;
    
    // åˆ é™¤æ–‡æ¡£
    sqlx::query("DELETE FROM knowledge_documents WHERE id = ?")
        .bind(&document_id)
        .execute(&pool)
        .await
        .map_err(|e| format!("åˆ é™¤çŸ¥è¯†åº“æ–‡æ¡£å¤±è´¥: {}", e))?;
    
    pool.close().await;
    Ok("æ–‡æ¡£åˆ é™¤æˆåŠŸ".to_string())
}

#[tauri::command]
async fn get_knowledge_statistics() -> Result<serde_json::Value, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    let doc_count: i64 = sqlx::query_scalar("SELECT COUNT(*) FROM knowledge_documents")
        .fetch_one(&pool)
        .await
        .map_err(|e| format!("è·å–æ–‡æ¡£æ•°é‡å¤±è´¥: {}", e))?;
    
    let vector_count: i64 = sqlx::query_scalar("SELECT COUNT(*) FROM vector_metadata")
        .fetch_one(&pool)
        .await
        .map_err(|e| format!("è·å–å‘é‡æ•°é‡å¤±è´¥: {}", e))?;
    
    let total_size: i64 = sqlx::query_scalar("SELECT COALESCE(SUM(file_size), 0) FROM knowledge_documents")
        .fetch_one(&pool)
        .await
        .map_err(|e| format!("è·å–æ€»å¤§å°å¤±è´¥: {}", e))?;
    
    pool.close().await;
    
    Ok(serde_json::json!({
        "documentCount": doc_count,
        "vectorCount": vector_count,
        "totalSize": total_size
    }))
}

#[tauri::command]
async fn generate_document_embeddings(document_id: String) -> Result<String, String> {
    // ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨å¹¶è·å–è·¯å¾„
    let data_dir = ensure_data_directory().await?;
    let db_path = format!("{}/ai_chat.db", data_dir);
    let pool = SqlitePool::connect(&format!("sqlite://{}", db_path))
        .await
        .map_err(|e| format!("è¿æ¥æ•°æ®åº“å¤±è´¥: {}", e))?;
    
    // è·å–æ–‡æ¡£å†…å®¹
    let document: (String, String) = sqlx::query_as(
        "SELECT title, content FROM knowledge_documents WHERE id = ?"
    )
    .bind(&document_id)
    .fetch_one(&pool)
    .await
    .map_err(|e| format!("è·å–æ–‡æ¡£å¤±è´¥: {}", e))?;
    
    let (title, content) = document;
    
    // åˆ†å—å¤„ç†æ–‡æ¡£å†…å®¹
    let chunks = chunk_text(&content, 500, 50);
    
    if chunks.is_empty() {
        pool.close().await;
        return Ok("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— éœ€ç”Ÿæˆå‘é‡".to_string());
    }
    
    // å‡†å¤‡æ‰¹é‡å¤„ç†çš„æ–‡æœ¬
    let batch_texts: Vec<String> = chunks.iter()
        .map(|chunk| format!("{} {}", title, chunk))
        .collect();
    
    // ä½¿ç”¨é»˜è®¤çš„å¤šè¯­è¨€æ¨¡å‹æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
    let model_name = "BAAI/bge-m3";
    println!("ğŸ” ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹: {}", model_name);
    
    // æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
    let embeddings = match generate_siliconflow_batch_embeddings("dummy_key".to_string(), batch_texts.clone(), model_name.to_string()).await {
        Ok(embeddings) => embeddings,
        Err(e) => {
            println!("âŒ æ‰¹é‡åµŒå…¥ç”Ÿæˆå¤±è´¥: {}, é™çº§åˆ°å•ä¸ªå¤„ç†", e);
            // é™çº§åˆ°å•ä¸ªå¤„ç†
            let mut embeddings = Vec::new();
            for text in batch_texts {
                match generate_siliconflow_embedding("dummy_key".to_string(), text.clone(), model_name.to_string()).await {
                    Ok(embedding) => embeddings.push(embedding),
                    Err(e) => {
                        println!("âŒ å•ä¸ªåµŒå…¥ç”Ÿæˆå¤±è´¥: {}, ä½¿ç”¨å¤‡ç”¨æ–¹æ³•", e);
                        // å¤‡ç”¨æ–¹æ³•
                        let mut embedding = vec![0.0; 768];
                        let mut hash = 0u64;
                        for byte in text.bytes() {
                            hash = hash.wrapping_mul(31).wrapping_add(byte as u64);
                            let index = (hash as usize) % 768;
                            embedding[index] = (hash % 1000) as f32 / 1000.0 - 0.5;
                        }
                        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
                        if norm > 0.0 {
                            for val in &mut embedding {
                                *val /= norm;
                            }
                        }
                        embeddings.push(embedding);
                    }
                }
            }
            embeddings
        }
    };
    
    // æ‰¹é‡æ·»åŠ å‘é‡åˆ°æ•°æ®åº“
    for (i, (chunk, embedding)) in chunks.iter().zip(embeddings.iter()).enumerate() {
        let vector = KnowledgeVector {
            vector_id: format!("{}_chunk_{}", document_id, i),
            document_id: document_id.clone(),
            chunk_index: i as i32,
            chunk_text: chunk.clone(),
            embedding: embedding.clone(),
            created_at: chrono::Utc::now().timestamp_millis(),
        };
        
        add_knowledge_vector_internal(&pool, vector).await?;
    }
    
    pool.close().await;
    Ok(format!("æˆåŠŸç”Ÿæˆ {} ä¸ªå‘é‡åµŒå…¥ï¼ˆä½¿ç”¨æ¨¡å‹: {}ï¼‰", chunks.len(), model_name))
}

// å†…éƒ¨å‡½æ•°ï¼šæ·»åŠ å‘é‡åˆ°æ•°æ®åº“
async fn add_knowledge_vector_internal(pool: &SqlitePool, vector: KnowledgeVector) -> Result<(), String> {
    // å°†å‘é‡è½¬æ¢ä¸ºJSONæ ¼å¼
    let embedding_json = serde_json::to_string(&vector.embedding)
        .map_err(|e| format!("åºåˆ—åŒ–å‘é‡å¤±è´¥: {}", e))?;
    
    // æ’å…¥å‘é‡åˆ°è™šæ‹Ÿè¡¨
    let result = sqlx::query(
        "INSERT INTO knowledge_vectors (embedding) VALUES (?)"
    )
    .bind(&embedding_json)
    .execute(pool)
    .await
    .map_err(|e| format!("æ·»åŠ å‘é‡å¤±è´¥: {}", e))?;
    
    let rowid = result.last_insert_rowid();
    
    // æ’å…¥å…ƒæ•°æ®åˆ°å…ƒæ•°æ®è¡¨
    sqlx::query(
        "INSERT INTO vector_metadata (rowid, document_id, chunk_index, chunk_text, created_at) VALUES (?, ?, ?, ?, ?)"
    )
    .bind(rowid)
    .bind(&vector.document_id)
    .bind(vector.chunk_index)
    .bind(&vector.chunk_text)
    .bind(vector.created_at)
    .execute(pool)
    .await
    .map_err(|e| format!("æ·»åŠ å‘é‡å…ƒæ•°æ®å¤±è´¥: {}", e))?;
    
    Ok(())
}

// æ–‡æœ¬åˆ†å—å‡½æ•°
fn chunk_text(text: &str, chunk_size: usize, overlap: usize) -> Vec<String> {
    let mut chunks = Vec::new();
    let mut start = 0;
    
    while start < text.len() {
        let end = std::cmp::min(start + chunk_size, text.len());
        let mut chunk = text[start..end].to_string();
        
        // å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
        if end < text.len() {
            if let Some(last_sentence) = chunk.rfind('ã€‚') {
                if last_sentence > chunk_size / 2 {
                    chunk = chunk[..last_sentence + 1].to_string();
                    start = start + last_sentence + 1 - overlap;
                } else {
                    start = end - overlap;
                }
            } else {
                start = end - overlap;
            }
        } else {
            start = end;
        }
        
        if !chunk.trim().is_empty() {
            chunks.push(chunk.trim().to_string());
        }
    }
    
    chunks
}

