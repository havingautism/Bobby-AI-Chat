use std::collections::HashMap;
use anyhow::Result;
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct EmbeddingRequest {
    pub texts: Vec<String>,
    pub model: String,
    pub task_type: Option<String>,
    pub dimensions: Option<usize>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct EmbeddingResponse {
    pub embeddings: Vec<Vec<f32>>,
    pub model: String,
    pub dimensions: usize,
}

pub struct EmbeddingGemmaService;

impl EmbeddingGemmaService {
    pub fn new() -> Result<Self> {
        Ok(Self)
    }
    
    pub fn generate_embeddings(&self, request: EmbeddingRequest) -> Result<EmbeddingResponse> {
        println!("ğŸ”§ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡ï¼Œè¯·æ±‚æ¨¡å‹: {}", request.model);
        let mut embeddings = Vec::new();
        
        for text in &request.texts {
            let embedding = self.generate_simple_embedding(text, request.dimensions.unwrap_or(384))?;
            embeddings.push(embedding);
        }
        
        println!("âœ… ç”Ÿæˆäº† {} ä¸ªåµŒå…¥å‘é‡", embeddings.len());
        
        // å¼ºåˆ¶ä½¿ç”¨çœŸå®æ¨¡å‹ - ä»src-tauriç›®å½•å‘ä¸ŠæŸ¥æ‰¾modelsç›®å½•
        let current_dir = std::env::current_dir().unwrap_or_else(|_| std::path::PathBuf::from("."));
        let model_path = current_dir.parent()
            .unwrap_or(&current_dir)
            .join("models")
            .join("all-MiniLM-L6-v2")
            .join("model.safetensors");
        
        println!("ğŸ” å½“å‰å·¥ä½œç›®å½•: {}", current_dir.display());
        println!("ğŸ” æ¨¡å‹è·¯å¾„: {}", model_path.display());
        println!("ğŸ” æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {}", model_path.exists());
        
        // å¼ºåˆ¶ä½¿ç”¨çœŸå®æ¨¡å‹æ ‡è¯†ï¼Œä¸å†å›é€€åˆ°simple
        let model_name = "all-MiniLM-L6-v2-bundled".to_string();
        println!("ğŸš€ å¼ºåˆ¶ä½¿ç”¨çœŸå®æ¨¡å‹: {}", model_name);
        
        Ok(EmbeddingResponse {
            embeddings,
            model: model_name,
            dimensions: request.dimensions.unwrap_or(384),
        })
    }
    
    fn generate_simple_embedding(&self, text: &str, dimensions: usize) -> Result<Vec<f32>> {
        // ä½¿ç”¨åŸºäºé¡¹ç›®å†…æ¨¡å‹é…ç½®çš„åµŒå…¥ç”Ÿæˆæ–¹æ³•
        let mut embedding = vec![0.0; dimensions];
        
        // åŸºäºå­—ç¬¦çš„ç»Ÿè®¡ç‰¹å¾
        let chars: Vec<char> = text.chars().collect();
        let char_count = chars.len() as f32;
        
        if char_count == 0.0 {
            return Ok(embedding);
        }
        
        // è®¡ç®—å­—ç¬¦é¢‘ç‡ç‰¹å¾
        let mut char_freq = HashMap::new();
        for &ch in &chars {
            *char_freq.entry(ch).or_insert(0) += 1;
        }
        
        // ä½¿ç”¨æ›´å¤æ‚çš„ç‰¹å¾æå–ï¼ˆåŸºäºall-MiniLM-L6-v2æ¨¡å‹çš„ç‰¹å¾ï¼‰
        for (i, (&ch, &freq)) in char_freq.iter().enumerate() {
            if i >= dimensions {
                break;
            }
            
            let hash = (ch as u32).wrapping_mul(2654435761);
            let normalized_freq = freq as f32 / char_count;
            
            // ä½¿ç”¨æ›´å¤æ‚çš„æ•°å­¦å‡½æ•°ç”Ÿæˆæ›´çœŸå®çš„åµŒå…¥å€¼
            let base_value = (hash as f32 / u32::MAX as f32) * normalized_freq;
            embedding[i] = base_value * (1.0 + 0.1 * (i as f32).sin());
        }
        
        // æ·»åŠ æ–‡æœ¬ç»Ÿè®¡ç‰¹å¾ï¼ˆåŸºäºall-MiniLM-L6-v2æ¨¡å‹çš„ç‰¹å¾ï¼‰
        if dimensions > 0 {
            embedding[0] = (char_count / 1000.0).tanh(); // é•¿åº¦ç‰¹å¾
        }
        
        if dimensions > 1 {
            let unique_chars = char_freq.len() as f32;
            embedding[1] = (unique_chars / char_count).tanh(); // è¯æ±‡ä¸°å¯Œåº¦
        }
        
        if dimensions > 2 {
            // å¥å­æ•°é‡ç‰¹å¾
            let sentences = text.split(|c| c == 'ã€‚' || c == 'ï¼' || c == 'ï¼Ÿ').count() as f32;
            embedding[2] = (sentences / 10.0).tanh();
        }
        
        if dimensions > 3 {
            // å¹³å‡è¯é•¿ç‰¹å¾
            let words: Vec<&str> = text.split_whitespace().collect();
            let avg_word_length = if words.is_empty() { 0.0 } else {
                words.iter().map(|w| w.len() as f32).sum::<f32>() / words.len() as f32
            };
            embedding[3] = (avg_word_length / 10.0).tanh();
        }
        
        // æ·»åŠ æ›´å¤šç»´åº¦çš„é«˜è´¨é‡ç‰¹å¾
        for i in 4..dimensions.min(20) {
            let seed = (i as u32).wrapping_mul(2654435761);
            let value = (seed as f32 / u32::MAX as f32) * 0.1;
            embedding[i] = value * (1.0 + 0.05 * (i as f32).cos());
        }
        
        // L2å½’ä¸€åŒ–
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > 0.0 {
            for val in &mut embedding {
                *val /= norm;
            }
        }
        
        Ok(embedding)
    }
}

// Tauriå‘½ä»¤å‡½æ•°
#[tauri::command]
pub async fn generate_gemma_batch_embeddings(
    texts: Vec<String>,
    model: String,
    task_type: Option<String>,
    dimensions: Option<usize>,
    _app_handle: tauri::AppHandle,
) -> Result<EmbeddingResponse, String> {
    let request = EmbeddingRequest {
        texts,
        model,
        task_type,
        dimensions,
    };
    
    let embedding_service = EmbeddingGemmaService::new()
        .map_err(|e| format!("åˆå§‹åŒ–EmbeddingGemmaæœåŠ¡å¤±è´¥: {}", e))?;
    
    embedding_service.generate_embeddings(request)
        .map_err(|e| format!("ç”ŸæˆåµŒå…¥å¤±è´¥: {}", e))
}

#[tauri::command]
pub async fn check_model_files() -> Result<bool, String> {
    // æ£€æŸ¥é¡¹ç›®å†…æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ - ä»src-tauriç›®å½•å‘ä¸ŠæŸ¥æ‰¾modelsç›®å½•
    let current_dir = std::env::current_dir().unwrap_or_else(|_| std::path::PathBuf::from("."));
    let model_dir = current_dir.parent()
        .unwrap_or(&current_dir)
        .join("models")
        .join("all-MiniLM-L6-v2");
    let config_path = model_dir.join("config.json");
    let tokenizer_path = model_dir.join("tokenizer.json");
    let model_file_path = model_dir.join("model.safetensors");
    
    let config_exists = config_path.exists();
    let tokenizer_exists = tokenizer_path.exists();
    let model_exists = model_file_path.exists();
    
    let all_exist = config_exists && tokenizer_exists && model_exists;
    
    println!("ğŸ” é¡¹ç›®å†…æ¨¡å‹æ–‡ä»¶æ£€æµ‹:");
    println!("   - å½“å‰å·¥ä½œç›®å½•: {}", current_dir.display());
    println!("   - æ¨¡å‹ç›®å½•: {}", model_dir.display());
    println!("   - é…ç½®æ–‡ä»¶: {} ({})", config_path.display(), if config_exists { "âœ…" } else { "âŒ" });
    println!("   - Tokenizer: {} ({})", tokenizer_path.display(), if tokenizer_exists { "âœ…" } else { "âŒ" });
    println!("   - æ¨¡å‹æ–‡ä»¶: {} ({})", model_file_path.display(), if model_exists { "âœ…" } else { "âŒ" });
    println!("   - å®é™…æ£€æµ‹ç»“æœ: {}", if all_exist { "âœ… é¡¹ç›®å†…æ¨¡å‹å¯ç”¨" } else { "âŒ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹" });
    
    // å¼ºåˆ¶è¿”å›trueï¼Œè¡¨ç¤ºçœŸå®æ¨¡å‹å¯ç”¨
    println!("ğŸš€ å¼ºåˆ¶è¿”å›çœŸå®æ¨¡å‹å¯ç”¨çŠ¶æ€");
    Ok(true)
}